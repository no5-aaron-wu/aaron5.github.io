<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>AI工具 [1]：AmbaPytorch量化重训 | 旭穹の陋室</title><meta name="robots" content="noindex"><meta name="keywords" content="AI,Tool,Amba,Quantization"><meta name="author" content="旭穹"><meta name="copyright" content="旭穹"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="前言 AmbaQuant-Pytorch（AQ-Pytorch）是Pytorch的优化工具。它可以执行与Amba CVflow和量化感知训练(Quantization-Aware Training，QAT)相同的量化行为，以恢复量化模型的精度。支持对稀疏化（剪枝）模型进行再训练，并保持模型的稀疏性。 安装 windows 安装pytorch 新建python3.7的conda虚拟环境 conda">
<meta property="og:type" content="article">
<meta property="og:title" content="AI工具 [1]：AmbaPytorch量化重训">
<meta property="og:url" content="https://no5-aaron-wu.github.io/2022/01/04/AI-Tools-1-AmbaPytorchQuant/index.html">
<meta property="og:site_name" content="旭穹の陋室">
<meta property="og:description" content="前言 AmbaQuant-Pytorch（AQ-Pytorch）是Pytorch的优化工具。它可以执行与Amba CVflow和量化感知训练(Quantization-Aware Training，QAT)相同的量化行为，以恢复量化模型的精度。支持对稀疏化（剪枝）模型进行再训练，并保持模型的稀疏性。 安装 windows 安装pytorch 新建python3.7的conda虚拟环境 conda">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://no5-aaron-wu.github.io/images/%E6%88%98%E5%8F%8CCG_%E8%A1%80%E6%88%98.png">
<meta property="article:published_time" content="2022-01-04T10:48:19.000Z">
<meta property="article:modified_time" content="2022-09-25T05:54:55.093Z">
<meta property="article:author" content="旭穹">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Tool">
<meta property="article:tag" content="Amba">
<meta property="article:tag" content="Quantization">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://no5-aaron-wu.github.io/images/%E6%88%98%E5%8F%8CCG_%E8%A1%80%E6%88%98.png"><link rel="shortcut icon" href="/images/ania_128x128.ico"><link rel="canonical" href="https://no5-aaron-wu.github.io/2022/01/04/AI-Tools-1-AmbaPytorchQuant/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="zlQaq8XLp3Oj3DgDnpkA0mCzGbGvflp_U-vZVql_E-E"/><meta name="msvalidate.01" content="EAE2F4C7507064F526B5924DA5DCD005"/><meta name="baidu-site-verification" content="code-XsSJQ4q5sT"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-6DDC82R7DV"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6DDC82R7DV');
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#009f6a","bgDark":"#121212","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'AI工具 [1]：AmbaPytorch量化重训',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2022-09-25 05:54:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mouse.css"><style>#article-container.post-content h1:before, h2:before, h3:before, h4:before, h5:before, h6:before { -webkit-animation: avatar_turn_around 1s linear infinite; -moz-animation: avatar_turn_around 1s linear infinite; -o-animation: avatar_turn_around 1s linear infinite; -ms-animation: avatar_turn_around 1s linear infinite; animation: avatar_turn_around 1s linear infinite; }</style><link rel="preconnect" href="https://fonts.gstatic.com"><link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@1,300&display=swap" rel="stylesheet"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="wizard-scene"><div class="wizard-objects"><div class="wizard-square"></div><div class="wizard-circle"></div><div class="wizard-triangle"></div></div><div class="wizard"><div class="wizard-body"></div><div class="wizard-right-arm"><div class="wizard-right-hand"></div></div><div class="wizard-left-arm"><div class="wizard-left-hand"></div></div><div class="wizard-head"><div class="wizard-beard"></div><div class="wizard-face"><div class="wizard-adds"></div></div><div class="wizard-hat"><div class="wizard-hat-of-the-hat"></div><div class="wizard-four-point-star --first"></div><div class="wizard-four-point-star --second"></div><div class="wizard-four-point-star --third"></div></div></div></div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">53</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/%E6%88%98%E5%8F%8CCG_%E8%A1%80%E6%88%98.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">旭穹の陋室</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">AI工具 [1]：AmbaPytorch量化重训</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-01-04T10:48:19.000Z" title="发表于 2022-01-04 10:48:19">2022-01-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-25T05:54:55.093Z" title="更新于 2022-09-25 05:54:55">2022-09-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="AI工具 [1]：AmbaPytorch量化重训"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>前言</h1>
<p>AmbaQuant-Pytorch（AQ-Pytorch）是Pytorch的优化工具。它可以执行与Amba CVflow和量化感知训练(Quantization-Aware Training，QAT)相同的量化行为，以恢复量化模型的精度。支持对稀疏化（剪枝）模型进行再训练，并保持模型的稀疏性。</p>
<h1>安装</h1>
<h2 id="windows">windows</h2>
<h3 id="安装pytorch">安装pytorch</h3>
<p>新建python3.7的conda虚拟环境</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda info -e</span><br><span class="line">conda create -c conda-forge -n python3.7 python=3.7</span><br><span class="line">conda info -e</span><br></pre></td></tr></table></figure>
<blockquote>
<p># conda environments:<br>
#<br>
base                  *  D:\software\anaconda3<br>
python3.6                D:\software\anaconda3\envs\python3.6<br>
python3.7                D:\software\anaconda3\envs\python3.7</p>
</blockquote>
<p>按照文档提示安装指定版本的pytorch包（不要用powershell，不支持conda虚拟环境）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda activate python3.7</span><br><span class="line">conda install pytorch==1.3.0 torchvision==0.4.1 cudatoolkit=10.2 -c pytorch -n python3.7</span><br></pre></td></tr></table></figure>
<p>这样安装的pytorch不支持CUDA，改用<a target="_blank" rel="noopener" href="https://download.pytorch.org/whl/cu102/torch-1.6.0-cp37-cp37m-win_amd64.whl">whl文件</a>安装试试：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda uninstall pytorch torchvision</span><br><span class="line">pip install torch-1.6.0-cp37-cp37m-win_amd64.whl</span><br></pre></td></tr></table></figure>
<p>pytorch1.6.0对应的torchvision版本为0.7.0，同样用<a target="_blank" rel="noopener" href="https://download.pytorch.org/whl/cu102/torchvision-0.7.0-cp37-cp37m-win_amd64.whl">whl文件</a>安装（不过没有找到cu102的版本，先只装cpu版本）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install torchvision-0.7.0-cp37-cp37m-win_amd64.whl</span><br></pre></td></tr></table></figure>
<p>测试安装情况：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="built_in">print</span>(torch.__version__)</span><br><span class="line">    <span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line">    <span class="built_in">print</span>(torchvision.__version__)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>1.6.0<br>
True<br>
0.7.0</p>
</blockquote>
<p>安装其他包：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install opencv-python</span><br><span class="line">pip install visdom</span><br></pre></td></tr></table></figure>
<h3 id="配置环境">配置环境</h3>
<p>Pycharm新建工程</p>
<p><img src="/2022/01/04/AI-Tools-1-AmbaPytorchQuant/1.png" alt="1"></p>
<p>按原文档教程，是执行<code>env_set.sh</code>脚本export全局变量，对应到Windows就是配置全局环境变量<code>$PYTHONPATH</code>，这里我不想动我的环境变量，就只配置pycharm的Interpreter：</p>
<ul>
<li>将<code>ambatrainingtool</code>文件夹拷贝到<code>D:\software\anaconda3\envs\python3.7\Lib\site-packages</code>下（放到其他非中文目录也行，这里方便管理）；</li>
<li>按照下图所示配置Interpreter：</li>
</ul>
<p><img src="/2022/01/04/AI-Tools-1-AmbaPytorchQuant/2.png" alt="2"></p>
<h2 id="linux（伪）">linux（伪）</h2>
<h3 id="准备镜像">准备镜像</h3>
<p>由于CVTOOLS需要在安霸的SDK环境下使用，<s>所以又需要在10.0.62.9服务器上搭建环境</s>，搭一半发现10.0.62.9服务器没有独立显卡，所以又选择本地搭建。</p>
<p>首先将包含SDK的镜像导出成tar文件然后拷贝到本地：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker save -o insta360_sdk8.tar insta360_sdk8:v1.0</span><br></pre></td></tr></table></figure>
<p>然后本地加载镜像：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">docker load <span class="literal">-i</span> insta360_sdk8.tar</span><br></pre></td></tr></table></figure>
<h3 id="系统升级">系统升级</h3>
<p>Windows下docker使用linux的镜像都是通过WSL2后端实现的，而WSL2想要使用GPU则需要安装CUDA for WSL驱动，而想要能正常安装这个驱动，则需要更新Win10到较新的版本（网传Build 20145或更高）。那就更新吧，正好我的WSL2也想用CUDA来着。</p>
<p>在系统设置里更新windows到最新版本：</p>
<p><img src="/2022/01/04/AI-Tools-1-AmbaPytorchQuant/4.png" alt="4"></p>
<p>然后安装<a target="_blank" rel="noopener" href="https://developer.nvidia.cn/51006-gameready-win11-win10-dch-64bit-international">驱动</a>，一路默认就行。</p>
<h3 id="清理镜像自带CUDA">清理镜像自带CUDA</h3>
<p>用包含有SDK的镜像新建一个docker容器并打开：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">docker container run <span class="literal">-itd</span> -<span class="literal">-name</span> cv22_sdk8_aq_torch insta360_sdk8:v1.<span class="number">0</span></span><br><span class="line">docker <span class="built_in">ps</span> <span class="literal">-a</span></span><br><span class="line">docker <span class="built_in">start</span> cv22_sdk8_aq_torch </span><br><span class="line">docker exec <span class="literal">-it</span> cv22_sdk8_aq_torch /bin/bash</span><br></pre></td></tr></table></figure>
<p>镜像自带了python3.7的环境，且安装了torch1.1.0，cuda版本为9.1，找不到合适的torch+cu版本（至少要cu92）。</p>
<p>先简单验证一下docker是否能穿透到外面获取设备信息：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">lspci -v | grep 3D</span><br></pre></td></tr></table></figure>
<p>如果没装cuda for wsl驱动，该命令不会打印任何东西，有以下输出说明驱动大概是没问题。</p>
<blockquote>
<p>158b:00:00.0 3D controller: Microsoft Corporation Device 008e<br>
792b:00:00.0 3D controller: Microsoft Corporation Device 008e</p>
</blockquote>
<p>尝试更新CUDA版本，首先查看已安装的包：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dpkg -l | grep cuda</span><br></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<blockquote>
<p>ii  libcudart9.1:amd64                     9.1.85-3ubuntu1                     amd64        NVIDIA CUDA Runtime Library<br>
ii  nvidia-cuda-dev                        9.1.85-3ubuntu1                     amd64        NVIDIA CUDA development files<br>
ii  nvidia-cuda-doc                        9.1.85-3ubuntu1                     all          NVIDIA CUDA and OpenCL documentation<br>
ii  nvidia-cuda-gdb                        9.1.85-3ubuntu1                     amd64        NVIDIA CUDA Debugger (GDB)<br>
ii  nvidia-cuda-toolkit                    9.1.85-3ubuntu1                     amd64        NVIDIA CUDA development toolkit</p>
</blockquote>
<p>根据包名字卸载包：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apt-get remove libcuda*</span><br><span class="line">apt-get remove nvidia-cuda*</span><br><span class="line">apt autoremove</span><br><span class="line">dpkg -l | grep cuda</span><br></pre></td></tr></table></figure>
<p>显示如下：</p>
<blockquote>
<p>rc  nvidia-cuda-toolkit                    9.1.85-3ubuntu1                     amd64        NVIDIA CUDA development toolkit</p>
</blockquote>
<p>其实这个包已经卸载了，只是还在源里，要想彻底点就执行：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dpkg -P nvidia-cuda-toolkit</span><br><span class="line">dpkg -l | grep cuda</span><br></pre></td></tr></table></figure>
<p>输出就干净了。<code>nvcc -V</code>也不能再打印版本号。</p>
<h3 id="升级WIN11">升级WIN11</h3>
<p>这时我又发现想要docker使用GPU必须使用<code>--gpus</code>命令启动容器，否则不能正常加载驱动（虽然CUDA能正常安装）：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">docker run <span class="literal">-it</span> -<span class="literal">-gpus</span> all -<span class="literal">-name</span> &lt;container name&gt; insta360_sdk8:v1.<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>而我加上这条命令后就会如下报错：</p>
<blockquote>
<p>stderr: nvidia-container-cli: initialization error: driver error: failed to process request: unknown</p>
</blockquote>
<p>google大概的意思是原生docker for windows支持GPU是最近一年的事，所以对系统版本的要求比较高，即便我用了最新的正式版系统也不行，老哥们给的建议是要么加入win10预览版体验计划，要么升级win11。大概了解了一下win11的兼容性之后，一不做二不休，我索性升win11了。</p>
<p>一顿折腾后升到了win11，再次执行：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">docker run <span class="literal">-it</span> -<span class="literal">-gpus</span> all -<span class="literal">-name</span> &lt;container name&gt; insta360_sdk8:v1.<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>报错又变了：</p>
<blockquote>
<p>stderr: nvidia-container-cli: mount error: file creation failed: /var/lib/docker/overlay2/4c642b74fe9a316fb16402ec592018dc2643826b8f0c4df74e0a97db60e4a766/merged/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1: file exists: unknown.</p>
</blockquote>
<p>一顿google，<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/nvidia-docker/issues/1551">这里</a>的老哥说是镜像里还有有一些ghost file没卸载干净，于是：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dpkg -l | grep libnvidia*</span><br></pre></td></tr></table></figure>
<p>果然还有：</p>
<blockquote>
<p>ii  libnvidia-cfg1-390:amd64               390.138-0ubuntu0.18.04.1            amd64        NVIDIA binary OpenGL/GLX configuration library<br>
ii  libnvidia-common-390                   390.138-0ubuntu0.18.04.1            all          Shared files used by the NVIDIA libraries<br>
ii  libnvidia-compute-390:amd64            390.138-0ubuntu0.18.04.1            amd64        NVIDIA libcompute package<br>
ii  libnvidia-decode-390:amd64             390.138-0ubuntu0.18.04.1            amd64        NVIDIA Video Decoding runtime libraries<br>
ii  libnvidia-encode-390:amd64             390.138-0ubuntu0.18.04.1            amd64        NVENC Video Encoding runtime library<br>
ii  libnvidia-fbc1-390:amd64               390.138-0ubuntu0.18.04.1            amd64        NVIDIA OpenGL-based Framebuffer Capture runtime library<br>
ii  libnvidia-gl-390:amd64                 390.138-0ubuntu0.18.04.1            amd64        NVIDIA OpenGL/GLX/EGL/GLES GLVND libraries and Vulkan ICD<br>
ii  libnvidia-ifr1-390:amd64               390.138-0ubuntu0.18.04.1            amd64        NVIDIA OpenGL-based Inband Frame Readback runtime library</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apt-get remove libnvidia*</span><br><span class="line">dpkg -P libnvidia-compute-390:amd64</span><br><span class="line">apt autoremove</span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
<p>然后提交修改后的镜像：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">docker commit &lt;container id&gt; insta360_sdk8:v1.<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>再次执行：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">docker run <span class="literal">-it</span> -<span class="literal">-gpus</span> all -<span class="literal">-name</span> test insta360_sdk8:v1.<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>能成功启动容器了。</p>
<h3 id="安装CUDA10-2">安装CUDA10.2</h3>
<p>然后准备安装cuda10.2，执行如下操作：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run</span><br><span class="line">sh cuda_10.2.89_440.33.01_linux.run</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/04/AI-Tools-1-AmbaPytorchQuant/5.png" alt="5"></p>
<p>输入<code>accept</code>：</p>
<p><img src="/2022/01/04/AI-Tools-1-AmbaPytorchQuant/6.png" alt="6"></p>
<p>记得取消掉驱动的<code>X</code>，不在容器内安装驱动，然后<code>Install</code>继续：</p>
<p>安装成功后有如下提示：</p>
<p><img src="/2022/01/04/AI-Tools-1-AmbaPytorchQuant/7.png" alt="7"></p>
<p>看提示需要添加环境变量，执行<code>vim ~/.bashrc</code>，输入以下内容：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_HOME=/usr/<span class="built_in">local</span>/cuda-10.2</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$&#123;CUDA_HOME&#125;</span>/lib64</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;CUDA_HOME&#125;</span>/bin:<span class="variable">$&#123;PATH&#125;</span></span><br></pre></td></tr></table></figure>
<p>再执行<code>source ~/.bashrc</code>使其生效。这时再执行<code>nvcc -V</code>就可以打印版本号了。</p>
<p>最后一步验证，跑一下NVIDIA的sample：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> NVIDIA_CUDA-10.2_Samples/1_Utilities/deviceQuery</span><br><span class="line">make</span><br><span class="line">./deviceQuery</span><br></pre></td></tr></table></figure>
<p>能正常打印GPU设备信息是为成功。</p>
<p><img src="/2022/01/04/AI-Tools-1-AmbaPytorchQuant/8.png" alt="8"></p>
<p>再commit一下：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">docker commit &lt;container id&gt; insta360_sdk8:v1.<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="安装pytorch-2">安装pytorch</h3>
<p>先卸载自带的pytorch包，再安装符合要求的torch版本（torch1.6.0+cu102，torchvision0.7.0+cu102）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip uninstall torch tensorflow  <span class="comment"># 卸载自带的torch</span></span><br><span class="line">pip install -U pip	<span class="comment"># pip 换源</span></span><br><span class="line">pip config <span class="built_in">set</span> global.index-url http://mirrors.aliyun.com/pypi/simple</span><br><span class="line">pip config <span class="built_in">set</span> install.trusted-host mirrors.aliyun.com</span><br><span class="line">pip install torch==1.6.0 torchvision==0.7.0 <span class="comment"># 安装要求版本</span></span><br><span class="line">python3</span><br><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; import torchvision</span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span>(torch.__version__)</span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span>(torchvision.__version__)</span><br></pre></td></tr></table></figure>
<p>能正确打印版本信息，且<code>torch.cuda.is_available()</code>返回True，是为安装成功。</p>
<h3 id="利用ONER2工程配置CVTOOLS环境">利用ONER2工程配置CVTOOLS环境</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir code</span><br><span class="line"><span class="built_in">cd</span> code</span><br><span class="line">git <span class="built_in">clone</span> https://gitlab.insta360.com/embedded/camera/ambarella/cv22/OneR2.git</span><br><span class="line">cp -r OneR2/rtos/cortex_a/build/maintenance/env/ ~/code/</span><br><span class="line"><span class="built_in">cd</span> ~/code/env/</span><br><span class="line"><span class="built_in">source</span> env_set.sh cv22 /opt/amba/</span><br></pre></td></tr></table></figure>
<p>有以下信息打印：</p>
<blockquote>
<p>Insert toolchain PATH: /usr/local/gcc-arm-none-eabi-9-2020-q2-update-amba-A_R-Profile_r1_20200804/bin and /usr/local/gcc-arm-9.2-2019.12-x86_64-aarch64-none-elf/bin<br>
renamed ‘/home/ambadocker/.tv2.cv22u’ -&gt; ‘/home/ambadocker/.tv2.cv22u.bak’<br>
Sepcify CVTOOLS version: “CVTOOLS sw-cv22.2.0.1.714 in /home/ambadocker/.tv2.cv22u”<br>
PROJECT=“cv22”<br>
AMBA_ROOT=“/opt/amba/”<br>
TV2_CONFIG=“/opt/amba//cv22/tv2/cv22/tv2config”<br>
CVTOOL_DIR=“/opt/amba//cv22/local/bin /opt/amba//cv22/tv2/exe /opt/amba//cv2/tv2/exe /opt/amba//cv2_third/tv2/exe /opt/amba//base/tv2/exe”<br>
Current Path:<br>
/opt/amba//base/tv2/exe:/opt/amba//cv2_third/tv2/exe:/opt/amba//cv2/tv2/exe:/opt/amba//cv22/tv2/exe:/opt/amba//cv22/local/bin:/usr/local/gcc-arm-none-eabi-9-2020-q2-update-amba-A_R-Profile_r1_20200804/bin:/usr/local/gcc-arm-9.2-2019.12-x86_64-aarch64-none-elf/bin:/usr/local/cuda-10.2/bin:/opt/caffe/build/tools:/opt/caffe/python:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</p>
</blockquote>
<p>测试一下能否使用CVTOOLS的脚本：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">graph_surgery.py</span><br></pre></td></tr></table></figure>
<p>能自动补全，且有以下打印信息是为配置成功。</p>
<blockquote>
<p>usage: graph_surgery.py {caffe,tf,onnx} …</p>
<p>Graph surgery tool: Edits model to make it VP friendly</p>
<p>positional arguments:<br>
{caffe,tf,onnx}  caffe/tf command help<br>
caffe          caffe graph surgery<br>
tf             tensorflow graph surgery<br>
onnx           onnx graph surgery<br>
graph_surgery.py &lt;caffe/tf/onnx&gt; -h</p>
</blockquote>
<h3 id="安装其他依赖">安装其他依赖</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install opencv-python</span><br><span class="line">pip install visdom</span><br></pre></td></tr></table></figure>
<h3 id="安装量化工具">安装量化工具</h3>
<p>拷贝量化工具到容器内，并打开容器：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">cp</span> .\AmbaTrainingTool_PyTorch_2.<span class="number">4.1</span>.<span class="number">06</span><span class="literal">-30</span><span class="literal">-2022</span>.tar.gz test:/home/ambadocker/code/</span><br><span class="line">docker <span class="built_in">start</span> test</span><br><span class="line">docker exec <span class="literal">-it</span> test /bin/bash</span><br></pre></td></tr></table></figure>
<p>解压压缩包，并执行工具下的<code>env_set.sh</code>脚本：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> code/</span><br><span class="line">tar zxvf AmbaTrainingTool_PyTorch_2.4.1.06-30-2022.tar.gz</span><br><span class="line"><span class="built_in">cd</span> ambatrainingtool/pytorch/</span><br><span class="line"><span class="built_in">source</span> env_set.sh</span><br></pre></td></tr></table></figure>
<p>可以用<code>echo $PYTHONPATH</code>查看python环境，也可以用执行如下指令来确认工具包能够被成功导入：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python3</span><br><span class="line">&gt;&gt;&gt; import ambarella.modules.all as amb</span><br></pre></td></tr></table></figure>
<h3 id="环境配置优化">环境配置优化</h3>
<p>上述方式配置CVTOOLS和量化工具的环境，当关闭容器重新打开后，又要重新配一遍，干脆将其加到<code>~/.bashrc</code>中：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim ~/.bashrc</span><br><span class="line"><span class="comment"># 文件最后添加以下内容</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cvtools env</span></span><br><span class="line"><span class="built_in">source</span> ~/code/env/env_set.sh cv22 /opt/amba &amp;&gt; /dev/null 2&gt;&amp;1</span><br><span class="line"><span class="comment"># aq-pytorch env</span></span><br><span class="line"><span class="built_in">source</span> ~/code/ambatrainingtool/pytorch/env_set.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>这样每次打开容器的时候就将环境配置好了，不用再重复操作。再<code>docker commit</code>一下，备个份。</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">docker commit &lt;container id&gt; insta360_sdk8:v1.<span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="vscode-docker（非必须）">vscode+docker（非必须）</h3>
<p>vscode安装<strong>Docker</strong>和<strong>Remote-Containers</strong>插件：</p>
<p><img src="/2022/01/04/AI-Tools-1-AmbaPytorchQuant/9.png" alt="9"></p>
<p><strong>Docker</strong>插件可以在侧边栏显示本地的容器和镜像情况：</p>
<p><img src="/2022/01/04/AI-Tools-1-AmbaPytorchQuant/10.png" alt="10"></p>
<p>而<strong>Remote-Containers</strong>则可以用来远程连接容器，最基本的用法就是连接一个正在运行的容器：</p>
<p><img src="/2022/01/04/AI-Tools-1-AmbaPytorchQuant/11.png" alt="11"></p>
<p>然后在容器上安装python插件，就可以调试python代码了。可以说方便很多了。</p>
<p><img src="/2022/01/04/AI-Tools-1-AmbaPytorchQuant/12.png" alt="12"></p>
<h1>示例数据下载</h1>
<p>为了能跑其提供的两个example，需要下载对应数据：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /&lt;yourpath&gt;/ambatrainingtool/pytorch/examples/0001_image_classification_PrAQ</span><br><span class="line">sh stanford_dogs.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /&lt;yourpath&gt;/ambatrainingtool/pytorch/examples/0002_object_detection_PrAQ/data</span><br><span class="line">sh VOC2007.sh .</span><br><span class="line">sh VOC2012.sh . </span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">sh download_weight.sh <span class="comment"># 需要给wget命令加上 --no-check-certificate 选项</span></span><br></pre></td></tr></table></figure>
<h1>量化工具</h1>
<p>如果不需要retraining，安霸的模型转换工具CNNGen中的训练后量化工具DRA（Data Range Analysis）可以为大多数网络提供令人满意的精度和可接受的性能。强制CNNGen使用FX8可以获得最好的性能，但它可能会损害精度。为了追求最佳的性能和保持良好的精度，可以基于更多的FX8数据来做QAT。</p>
<p>由于在安霸的CVflow上的推理不同于CPU/GPU，AQ-PyTorch提供了一些<strong>AQ算子</strong>来弥补训练时间（通常在CPU/GPU上）和推理时间（在CVflow上）之间的差距。有了这些AQ算子，用户在CPU/GPU上构建模型时，可以获得与在CVflow上推理近乎一致的推理结果。这有助于通过retraining来恢复量化所带来的精度损失。</p>
<p>为了用AQ算子替换PyTorch层，需要<code>CVTOOLS</code>来解析用户的模型。但是<code>CVTOOLS</code>并不直接支持PyTorch。因此，在使用<code>CVTOOLS</code>之前，需要将PyTorch模型导出为ONNX模型。为了便于理解，这里定义一些关于PyTorch和ONNX之间关系的术语：</p>
<ul>
<li>PyTorch模型的结构是基于分层模块的。PyTorch模型中的层称为<strong>模块</strong>（module）。</li>
<li>ONNX模型的结构是一个静态的<strong>图</strong>（graph）。ONNX图中的层称为<strong>节点</strong>（node）。</li>
<li><strong>叶模块</strong>（Leaf module）是一个没有子模块的模块，通常是Conv2d、Avg2dpool等。</li>
<li>所有的ONNX节点都应该一对一地映射到正确的PyTorch模块，否则就不能正确地用AQ算子替换PyTorch层。</li>
</ul>
<p>在使用AQ-PyTorch之前，应该首先执行初始训练（即从头开始的训练）。为了进行初始训练，应该准备好包含所有超参数的训练代码，并确保训练代码能够将模型训练到令人满意的精度。而后就可以使用AQ-PyTorch来量化模型并进行QAT。QAT的超参数是基于初始训练中的超参数，并且修改很少。</p>
<h2 id="总体流程">总体流程</h2>
<p>为了正确地使用AQ-PyTorch，需要遵循以下几个步骤，其中包括代码修改和CNNGen过程。通过这些步骤，可以从CNNGen中获得量化信息，并将PyTorch模型转换为AQ模型，AQ模型可以执行与CVflow相同的量化行为。共5个步骤如下：</p>
<ul>
<li>
<ol>
<li><strong>修改PyTorch模型代码</strong>为AQ-PyTorch能识别的样式。AQ-PyTorch主要用AQ算子取代PyTorch模块。模型代码应该遵循一些<a href="#%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81">书写规则</a>，使得AQ-PyTorch可以自动识别所有PyTorch模块，并用AQ算子替换它们。</li>
</ol>
</li>
<li>
<ol start="2">
<li><strong>修改ONNX模型导出代码</strong>。在运行CNNGen之前，需要将PyTorch模型导出为ONNX模型，因为CNNGen不直接支持PyTorch模型。要正确地导出ONNX模型，可以参照<a href="#ONNX%E5%AF%BC%E5%87%BA%E4%BB%A3%E7%A0%81%E9%9B%86%E6%88%90">这里</a>的伪代码。它可以确保ONNX节点与PyTorch模块有一对一的映射。一对一映射对于AQ-PyTorch将PyTorch模块转换为AQ算子非常重要。</li>
</ol>
</li>
<li>
<ol start="3">
<li>跑第一遍以<strong>获取AQ信息</strong>。这里包括导出ONNX模型、预处理ONNX模型和生成AQ信息。经过ONNX预处理后，CNNGen将分析给定的ONNX模型并生成AQ信息(sideband、vas、vlist等)。AQ信息用于建立AQ模型。</li>
</ol>
</li>
<li>
<ol start="4">
<li><strong>转换PyTorch模型到AQ模型</strong>。根据AQ信息，AQ-PyTorch可以用AQ算子替换所有的PyTorch模块，并在训练和测试时得到AQ模型。用户可以将AQ-PyTorch集成到他们的训练/测试代码中，进行QAT并恢复量化损失。</li>
</ol>
</li>
<li>
<ol start="5">
<li>跑第二遍以<strong>部署</strong>。这里与第一次的过程几乎相同。包括导出ONNX模型，并根据重新训练的PyTorch模型中对ONNX模型进行预处理。最后，CNNGen生成AQ信息，并将其部署到Ambarella的CVflow上。</li>
</ol>
</li>
</ul>
<h2 id="修改模型代码">修改模型代码</h2>
<p><code>0001_image_classification_PrAQ/model</code>目录下有两个模型代码<code>inceptionv3_original.py</code>和<code>inceptionv3.py</code>，分别是修改前后的代码。可以用来对比做了哪些修改。主要需要关注以下几点：</p>
<ul>
<li>将共享权重层修改为非共享权重层；</li>
<li>将无参数层修改为<code>__init__</code>中的模块定义；</li>
<li>将张量运算操作修改为<code>__init__</code>中的模块定义；</li>
<li>将模块列表（Python List）修改为PyTorch的<code>ModuleList</code>；</li>
<li>在量化再训练时，消除用户自定义的decoder层。</li>
</ul>
<h3 id="修改共享权重层或重用层">修改共享权重层或重用层</h3>
<p>AQ-PyTorch不支持共享权重层和层重用。因为AQ-PyTorch需要在PyTorch模块和ONNX节点之间的一对一映射，所以如果重用了同一层，则需要为每个调用重新定义一个新的层。如下：</p>
<ul>
<li>old</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN1</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN1, self).__init__()</span><br><span class="line">        self.Conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.Conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.Conv1(x)</span><br><span class="line">        x = self.Conv2(x)</span><br><span class="line">        x = self.Conv2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li>modify</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN1</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN1, self).__init__()</span><br><span class="line">        self.Conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.Conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.Conv3 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.Conv1(x)</span><br><span class="line">        x = self.Conv2(x)</span><br><span class="line">        x = self.Conv3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="修改无参数层">修改无参数层</h3>
<p>像池化层和激活层这些没有训练参数的层可以直接在<code>forward</code>函数中调用<code>torch.nn.functional.avg_pool2d(x, kernel_size=3)</code>。但是为了能够将PyTorch模块和ONNX节点之间进行一对一映射，需要将这些层以Module的形式定义到<code>__init__</code>函数中，同样的，不能存在重用层。</p>
<ul>
<li>old</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN2</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN2, self).__init__()</span><br><span class="line">        self.Conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.Conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.Conv1(x)</span><br><span class="line">        x = self.Conv2(x)</span><br><span class="line">        x = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        y = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li>modify</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN2</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN2, self).__init__()</span><br><span class="line">        self.Conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.Conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.Pool1 = nn.AvgPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.Pool2 = nn.AvgPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.Relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.Conv1(x)</span><br><span class="line">        x = self.Conv2(x)</span><br><span class="line">        x = self.Pool1(x)</span><br><span class="line">        y = self.Pool2(x)</span><br><span class="line">        x = self.Relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="修改张量运算操作">修改张量运算操作</h3>
<p>如<code>concat</code>、<code>add</code>、<code>mul</code>和<code>interpolate</code>等张量运算操作也需要定义为<code>__init__</code>函数中的模块。但是PyTorch本身并没有将这些操作实现为<code>torch.nn</code>中的module，因此安霸自己实现了这些op的模块。同样，为了一对一映射，不能重用模块。</p>
<ul>
<li>old</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN3</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN3, self).__init__()</span><br><span class="line">        self.Conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.Conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.Conv1(x)</span><br><span class="line">        x = self.Conv2(x)</span><br><span class="line">        x = torch.cat([x, x], <span class="number">1</span>)</span><br><span class="line">        x = torch.cat([x, x], <span class="number">1</span>)</span><br><span class="line">        y = x + x</span><br><span class="line">        y = y * y</span><br><span class="line">        y = F.interpolate(y, size=(<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<ul>
<li>modify</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN3</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN3, self).__init__()</span><br><span class="line">        self.Conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.Conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.Cat1 = amb.Concat()</span><br><span class="line">        self.Cat2 = amb.Concat()</span><br><span class="line">        self.Add = amb.Add()</span><br><span class="line">        self.Mul = amb.Mul()</span><br><span class="line">        self.Resize = amb.Interpolate()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.Conv1(x)</span><br><span class="line">        x = self.Conv2(x)</span><br><span class="line">        x = self.Cat1((x, x), <span class="number">1</span>)</span><br><span class="line">        x = self.Cat2((x, x), <span class="number">1</span>)</span><br><span class="line">        y = self.Add(x, x)</span><br><span class="line">        y = self.Mul(y, y)</span><br><span class="line">        y = self.Resize(y, size=(<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h3 id="修改模块列表">修改模块列表</h3>
<p>如果使用Python List作为层的容器，AQ-PyTorch将无法定位这些层，从而无法转换成AQ算子。因此建议使用PyTorch的ModuleList。如果已经有训练好的使用了Python List的模型，并且保存了权重，将模型改成ModuleList形式则可能会导致权重加载失败，因为名称不匹配。这时需要手动修改权重的名称，以能够映射到当前模型中的名称。一个简单的例子如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># Define NN use Python List and PyTorch ModuleList</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NN_List</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NN_List, self).__init__()</span><br><span class="line">        self.layers = [</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">21</span>, kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">1024</span>, <span class="number">21</span>, kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">21</span>, kernel_size=<span class="number">3</span>),</span><br><span class="line">         ]</span><br><span class="line">        <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.layers):</span><br><span class="line">            self.add_module(<span class="string">&quot;classifier_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i), l)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NN_ModuleList</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NN_ModuleList, self).__init__()</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">21</span>, kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">1024</span>, <span class="number">21</span>, kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">21</span>, kernel_size=<span class="number">3</span>),</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the model_List weight</span></span><br><span class="line">model_List = NN_List()</span><br><span class="line">state_dict_List = model_List.state_dict()</span><br><span class="line">torch.save(state_dict_List, <span class="string">&#x27;state_dict_List.pth&#x27;</span>)</span><br><span class="line"><span class="comment"># Modify the state_dict_List.pth</span></span><br><span class="line">pt_name =<span class="string">&#x27;state_dict_List&#x27;</span></span><br><span class="line">pth_name_in = <span class="string">&#x27;state_dict_List&#x27;</span> + <span class="string">&#x27;.pth&#x27;</span></span><br><span class="line">pth_name_out = <span class="string">&#x27;state_dict_ModuleList&#x27;</span> + <span class="string">&#x27;.pth&#x27;</span></span><br><span class="line">data = torch.load(pth_name_in)</span><br><span class="line"><span class="comment"># Users should build this map_table for their own model</span></span><br><span class="line">map_table = &#123;<span class="string">&#x27;classifier_0&#x27;</span>: <span class="string">&#x27;layers.0&#x27;</span>,</span><br><span class="line">             <span class="string">&#x27;classifier_1&#x27;</span>: <span class="string">&#x27;layers.1&#x27;</span>,</span><br><span class="line">             <span class="string">&#x27;classifier_2&#x27;</span>: <span class="string">&#x27;layers.2&#x27;</span>&#125;</span><br><span class="line"><span class="keyword">for</span> i, name_in <span class="keyword">in</span> <span class="built_in">enumerate</span>(map_table.keys()):</span><br><span class="line">    name_out = map_table[name_in]</span><br><span class="line">    old_name_group = [s <span class="keyword">for</span> i, s <span class="keyword">in</span> <span class="built_in">enumerate</span>(data.keys()) <span class="keyword">if</span> name_in <span class="keyword">in</span> s]</span><br><span class="line">    new_name_group = [tmp_str.replace(name_in, name_out) <span class="keyword">for</span> tmp_str <span class="keyword">in</span> old_name_group]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(old_name_group) != <span class="built_in">len</span>(new_name_group):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Error: Something wrong in mapping&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(old_name_group)</span><br><span class="line">        <span class="built_in">print</span>(new_name_group)</span><br><span class="line">        sys.exit(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(old_name_group)):</span><br><span class="line">        new_name = new_name_group[i]</span><br><span class="line">        old_name = old_name_group[i]</span><br><span class="line">        <span class="keyword">if</span> <span class="number">1</span>:</span><br><span class="line">            data[new_name] = data[old_name]</span><br><span class="line">            <span class="keyword">del</span> data[old_name]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data[new_name] = data.pop(old_name)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;update name from &#123;&#125; to &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(old_name, new_name))</span><br><span class="line">torch.save(data, pth_name_out)</span><br><span class="line"><span class="comment"># Load the modified state_dict</span></span><br><span class="line">model_ModuleList = NN_ModuleList()</span><br><span class="line">model_ModuleList.load_state_dict(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The model_ModuleList load state dict successfully&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="修改模型Decode层">修改模型Decode层</h3>
<p>一些模型（比如大多数的对象检测模型）具有一个自定义的Decode层（如NMS）。Decode层通常没有任何可学习的参数，所以retrain的时候可以没有。因为Decode层通常具有AQ-PyTorch不支持的自定义定义函数，所以如果模型具有这种Decode层，则在导出模型时要删掉Decode层，将Decode层之前一层的输出作为模型的输出。</p>
<p>可以定义一个导出flag来决定模型的输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> self.is_test:</span><br><span class="line">	confidences = self.softmax(confidences)、</span><br><span class="line">    <span class="keyword">if</span> self.porting: <span class="comment"># Exporting mode output</span></span><br><span class="line">        <span class="keyword">return</span> confidences.flatten(<span class="number">1</span>), locations.flatten(<span class="number">1</span>)</span><br><span class="line">    boxes = box_utils.convert_locations_to_boxes(</span><br><span class="line">        locations, self.priors, self.config.center_variance, self.config.size_variance</span><br><span class="line">    ) <span class="comment"># Decode layer</span></span><br><span class="line">    boxes = box_utils.center_form_to_corner_form(boxes) <span class="comment"># Decode layer</span></span><br><span class="line">    <span class="keyword">return</span> confidences, boxes <span class="comment"># Testing mode output</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> confidences, locations <span class="comment"># Training mode output</span></span><br></pre></td></tr></table></figure>
<h2 id="量化工具集成">量化工具集成</h2>
<h3 id="ONNX导出代码集成">ONNX导出代码集成</h3>
<p>可以使用安霸提供的<code>amba_onnx_export.py</code>脚本来完成PyTorch到ONNX模型的转换。也可以将导出模型的代码集成到自己的导出模型代码中。具体步骤如下伪代码所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> ambarella.onnx、</span><br><span class="line"><span class="keyword">import</span> ambarella.model <span class="keyword">as</span> amba</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Creating model!&quot;</span>)</span><br><span class="line"><span class="comment"># Step 1: Build model</span></span><br><span class="line"><span class="comment"># Insert your build model code here</span></span><br><span class="line">model = ... (use<span class="string">r&#x27;s model)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># Step 2: Load pretrained weight for fist pass</span></span><br><span class="line"><span class="string"># If it is first pass, load pretrained weight before merge bn.</span></span><br><span class="line"><span class="string">if args.pass_num == 1:</span></span><br><span class="line"><span class="string">	model.load_state_dict(pretrainded_model)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># Step 3: Convert to eval model</span></span><br><span class="line"><span class="string"># eval mode is necessary for ambarella quantization function</span></span><br><span class="line"><span class="string">model.eval()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># Step 4: Define your input size</span></span><br><span class="line"><span class="string">INPUT_SIZE = ... (input shape of user&#x27;</span>s model <span class="keyword">in</span> (N, D, H, W), e.g., (<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Check model graph and merge batchnorm</span></span><br><span class="line"><span class="comment"># Some traced scope names in the ONNX graph are incorrect, we need to override them</span></span><br><span class="line">(_<span class="keyword">pass</span>, graph) = amba.check(model, INPUT_SIZE, override_scopes=&#123;<span class="string">&#x27;900&#x27;</span>:<span class="string">&#x27;avg&#x27;</span>&#125;)</span><br><span class="line">model_mbn = amba.merge_bn(model, graph)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: Load pretrained weight for second pass</span></span><br><span class="line"><span class="comment"># If it is second pass, load quantized retrained weight after merge bn.</span></span><br><span class="line"><span class="keyword">if</span> args.pass_num == <span class="number">2</span>:</span><br><span class="line">	model.load_state_dict(pretrainded_model)</span><br><span class="line">                  </span><br><span class="line"><span class="comment"># Step 7: Check model graph after merging batchnorm</span></span><br><span class="line"><span class="comment"># After merge_bn, the graph is changed. the override_scopes are also changed</span></span><br><span class="line"><span class="comment"># Please find the correct node:module pair to override</span></span><br><span class="line">(_<span class="keyword">pass</span>, graph_mbn) = amba.check(model_mbn, INPUT_SIZE, override_scopes=&#123;<span class="string">&#x27;430&#x27;</span>:<span class="string">&#x27;avg&#x27;</span>&#125;)</span><br><span class="line"><span class="comment"># Step 8: Export model onnx</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(INPUT_SIZE)</span><br><span class="line">ambarella.onnx.export(model_mbn, <span class="built_in">input</span>, args.output_name, keep_initializers_as_inputs=<span class="literal">True</span>,</span><br><span class="line">                      input_names=[<span class="string">&#x27;input&#x27;</span>], output_names=[<span class="string">&#x27;output&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>其中Step1和Step8在自己的模型代码中应该已经存在（Step8需要修改成安霸的导出API），只需要添加Step2-Step7。其中关于<code>override_scopes</code>参数主要时为了对齐导出时映射错误的层，详细的解释详见原文档7.2节，这里就不展开。</p>
<h3 id="训练-测试代码集成">训练/测试代码集成</h3>
<p>大致步骤可以见如下伪代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import amba quant</span></span><br><span class="line"><span class="keyword">import</span> ambarella.model <span class="keyword">as</span> amba</span><br><span class="line"><span class="comment"># import pruner if your model is pruned</span></span><br><span class="line"><span class="keyword">from</span> ptpruner.Pytorch_pruner <span class="keyword">import</span> Pruner</span><br><span class="line"></span><br><span class="line"><span class="comment"># If users want to get deterministic test accuracy. Add these setting</span></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up model</span></span><br><span class="line">model = ... (use<span class="string">r&#x27;s model)</span></span><br><span class="line"><span class="string">model.load_state_dict(pretrainded_model)</span></span><br><span class="line"><span class="string">input_shape = ... (input shape of user&#x27;</span>s model <span class="keyword">in</span> (N, D, H, W), e.g., (<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Quantize model</span></span><br><span class="line"><span class="comment"># Suggest quantize model before DataParall or DistributedDataParallel</span></span><br><span class="line">model = amba.quantize(model, input_shape, [<span class="string">&#x27;input&#x27;</span>], [<span class="string">&#x27;output&#x27;</span>],</span><br><span class="line">		sideband = <span class="string">&#x27;/YOUR/PATH/model_sb.json&#x27;</span>,</span><br><span class="line">		imme_dir = <span class="string">&#x27;/YOUR/PATH/out&#x27;</span>,</span><br><span class="line">		primv = <span class="string">&#x27;/YOUR/PATH/model.vas&#x27;</span>,</span><br><span class="line">		vlist = <span class="string">&#x27;/YOUR/PATH/vas_output/model.vlist&#x27;</span>,</span><br><span class="line">		override_scopes = &#123;&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use multi-GPU</span></span><br><span class="line">model = torch.nn.DataParallel(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test model</span></span><br><span class="line"><span class="comment"># Here the model is quantized. Users can test the quantized model.</span></span><br><span class="line">Acc = test(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training Setup</span></span><br><span class="line"><span class="comment"># Please initial optimizer after quantization</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),</span><br><span class="line">                            lr=args.lr,</span><br><span class="line">                            momentum=args.momentum,</span><br><span class="line">                            weight_decay=args.weight_decay)</span><br><span class="line">                   </span><br><span class="line"><span class="comment"># Setup a pruner class if you are retraining a pruned model.</span></span><br><span class="line"><span class="comment"># The Pruner can just feed 1 (total_iter=1).</span></span><br><span class="line">pruner = Pruner(<span class="number">1</span>)</span><br><span class="line">statistics, _, _ = pruner.show_statistics(model,input_shape)</span><br><span class="line">pruner.pruned_model_analysis(model)</span><br><span class="line"><span class="keyword">for</span> curr_epoch <span class="keyword">in</span> <span class="built_in">range</span>(total_epochs):</span><br><span class="line">	<span class="keyword">for</span> curr_batch <span class="keyword">in</span> <span class="built_in">range</span>(total_batches_per_epoch):</span><br><span class="line">		<span class="comment"># data</span></span><br><span class="line">		<span class="built_in">input</span>, label = ... (use<span class="string">r&#x27;s dataset)</span></span><br><span class="line"><span class="string">		# Forward step</span></span><br><span class="line"><span class="string">		output = model(input)</span></span><br><span class="line"><span class="string">		# Loss computation and backward step</span></span><br><span class="line"><span class="string">		optimizer.zero_grad()</span></span><br><span class="line"><span class="string">		loss = criterion(outputs, label)</span></span><br><span class="line"><span class="string">		loss.backward()</span></span><br><span class="line"><span class="string">		optimizer.step()</span></span><br><span class="line"><span class="string">		# If you are retraining a pruned model, keep the model pruned</span></span><br><span class="line"><span class="string">		# before every forward step and after every backward step</span></span><br><span class="line"><span class="string">		pruner.keep_pruned(model)</span></span><br><span class="line"><span class="string">	if scheduler is not None:</span></span><br><span class="line"><span class="string">		scheduler.step()</span></span><br></pre></td></tr></table></figure>
<p>需要注意的点有：</p>
<ul>
<li>量化模型的精度因为cudnn的缘故可能会有一点随机，如果想要模型的精度是确定的，需要添加<code>torch.backends.cudnn.deterministic = True</code>和<code>torch.backends.cudnn.benchmark = False</code>；</li>
<li>如果模型是数据并行（DataParallel，DP）或者分布式数据并行（DistributedDataParallel）的，则需要在用<code>torch.nn.DataParallel</code>封装模型前执行量化操作，顺序错误可能会导致训练过程卡住；</li>
<li>在量化后执行优化器初始化操作，可以将PyTorch层转换为AQ算子。否则优化器中的参数是PyTorch层的参数；</li>
<li>当重训练一个做过稀疏化（剪枝）处理的模型时，需要构建一个修剪器（pruner）。pruner的输入参数<code>total_iter</code>只能设为1。在量化重训练场景下，只有<code>pruner.pruned_model_analysis</code>和<code>pruner.keep_pruned(model)</code>会被调用以维持模型的稀疏性，其他参数不会被用到。</li>
</ul>
<h3 id="重训练超参">重训练超参</h3>
<p>在重新训练量化模型之前，应该先有要进行初始训练的训练代码。一旦能够进行初始训练并得到一个令人满意的模型精度，就可以使用量化工具来进行再训练。</p>
<p>重训练使用与初始训练类似的超参数。但可以使用更少的训练周期（如1/5的初始训练周期）。如果精度恢复不够，用户可以尝试一个更低的学习率（如初始训练学习率的1/10）。</p>
<h2 id="量化模型重训练">量化模型重训练</h2>
<h3 id="量化重训流程">量化重训流程</h3>
<p>如前所述，AQ-PyTorch可以将模型中的PyTorch层转化为AQ算子，但需要通过CVTOOLS生成AQ信息（sideband等）。然而安霸的CVTOOLS不直接支持PyTorch模型，所以需要将PyTorch模型导出为ONNX模型。转化为AQ算子后，PyTorch模型就可以获得与在CVFlow上执行推理近乎一致的结果，这样就可以通过重训练来恢复因量化导致的精度损失。</p>
<p>量化重训的流程如下图所示（使用安霸提供的样例）：</p>
<p><img src="/2022/01/04/AI-Tools-1-AmbaPytorchQuant/3.png" alt="3"></p>
<p>整个流程大致可以分成两个阶段，两个阶段有一些公共的步骤，如导出ONNX模型，对ONNX模型进行预处理等。各个脚本的作用大致如下：</p>
<ul>
<li><code>amba_onnx_export.py</code>脚本接收一个经过预训练的模型作为输入，对模型做一些预处理（合并bn层），将模型导出为ONNX模型。大致流程类似<a href="#ONNX%E5%AF%BC%E5%87%BA%E4%BB%A3%E7%A0%81%E9%9B%86%E6%88%90">ONNX导出代码集成</a>；</li>
<li><code>cnngen_preproc.sh</code>脚本对ONNX模型再做一些预处理（将全局池化转化为非全局池化，移除一些常量节点）；</li>
<li><code>cnngen_pass1.sh</code>脚本会生成用于AQ算子的sideband信息；</li>
<li><code>exp_inception_testing_quant.py</code>脚本接收sideband信息作为输入，测试模型量化后的精度情况，基于此来决定是否需要retrain；</li>
<li><code>exp_inception_quant.py</code>脚本会retrain量化模型，得到量化模型重训练后的权重；</li>
<li><code>cnngen_pass2.sh</code>脚本用来部署重训练后的模型。</li>
</ul>
<h2 id="量化函数使用方法">量化函数使用方法</h2>
<p>为了用AQ算子替换PyTorch层，需要将量化函数集成到训练和测试代码中。主要的API就是<code>amba.quantize</code>，其他API在<code>amba.quantize</code>中调用或是在导出代码中调用。其中<code>import ambarella.model as amba</code>。</p>
<ul>
<li><code>amba.quantize</code>：量化PyTorch模型，参数详情见原文3.5.2；</li>
<li><code>amba.check</code>：检查PyTorch模型是否支持量化，首先调用<code>get_graph</code>生成ONNX图，然后检查所有的叶模块都能与ONNX节点一一对应；若某些模块没能映射成功，则会打印警告信息，如果这些模块确实是在eval阶段会用到，则需要自己找出映射关系，填到<code>override_scopes</code>中，来帮助工具映射。</li>
<li><code>amba.get_graph</code>：生产ONNX图；</li>
<li><code>amba.merge_bn</code>：将BN层合并到卷积层；原来的<code>BatchNorm</code>模块会被替换成<code>linear.Identity</code>模块，这些替换信息会作为log信息打印，注意有些BN层不是推理时的模块，可能就不会被替换；</li>
<li><code>amba.custom</code>：给定PyTorch模型、ONNX图和一些CVTools量化信息（sideband，vlist等），用AQ算子替换PyTorch层；AQ算子到Pytorch层的映射关系见原文7.3节，并不是所有的层都会被替换成AQ算子，只需要检查在映射表中列出的层都成功转换即可。当然，和上面一样，不在推理时使用的模块也不会被替换；</li>
<li><code>amba.compare</code>：比较两个PyTorch模型的输出的差异。这个函数可用于比较merge_bn之前和merge_bn之后的输出；</li>
<li><code>amba.match_ades</code>：逐层比较并检查量化的PyTorch模型输出和CVTools ades输出。这个函数是用于回归流程；</li>
<li><code>ambarella.onnx.export</code>：将PyTorch模型导出到ONNX模型中。与<code>torch.onnx.export</code>的接口参数一致，但只有部分参数可设置，其他均为默认值。具体见原文3.5.9节。</li>
</ul>
<h2 id="例子">例子</h2>
<p>按照如下步骤执行脚本：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># change directory</span></span><br><span class="line"><span class="built_in">cd</span> &lt;your path ambatrainingtool&gt;/pytorch/examples/0001_image_classification_PrAQ/inception_v3_dog/</span><br><span class="line"><span class="comment"># [PyTorch] build and preprocess a PyTorch model, then, export th an ONNX model.</span></span><br><span class="line">python3 amba_onnx_export.py --model ../well_trained/pruning/inception_v3_114.pth.tar --pass_num 1 --output_name usr_input/model_mbn.onnx</span><br><span class="line"><span class="comment"># [CVTools] preprocess the onnx model</span></span><br><span class="line">sh cnngen_preproc.sh usr_input/model_mbn.onnx usr_input/model_preproc.onnx</span><br><span class="line"><span class="comment"># [CVTools] generate sideband information</span></span><br><span class="line">sh cnngen_pass1.sh</span><br><span class="line"><span class="comment"># change directory</span></span><br><span class="line"><span class="built_in">cd</span> ../</span><br><span class="line"><span class="comment"># [PyTorch] test the quantized model accuracy</span></span><br><span class="line">python3 inception_v3_dog/exp_inception_testing_quant.py</span><br><span class="line"><span class="comment"># [PyTorch] quantizetion-aware training</span></span><br><span class="line">python3 inception_v3_dog/exp_inception_quant.py</span><br><span class="line"><span class="comment"># change directory</span></span><br><span class="line"><span class="built_in">cd</span> inception_v3_dog/</span><br><span class="line"><span class="comment"># [PyTorch] build and preprocess a PyTorch model, then, export th an ONNX model.</span></span><br><span class="line">python3 amba_onnx_export.py --model ../checkpoint/inception_v3_quant_release/inception_v3_20.pth.tar --pass_num 2 --output_name usr_input/model_quant_retrained_mbn.onnx</span><br><span class="line"><span class="comment"># [CVTools] preprocess the onnx model</span></span><br><span class="line">sh cnngen_preproc.sh usr_input/model_quant_retrained_mbn.onnx usr_input/model_quant_retrained_preproc.onnx</span><br><span class="line"><span class="comment"># [CVTools] deploy the retrained model</span></span><br><span class="line">sh cnngen_pass2.sh</span><br></pre></td></tr></table></figure>
<p>执行过程中会有一些log信息打印，其中可能会遇到如下问题：</p>
<ul>
<li>脚本中包含<code>amba.check</code>的话，log中会包含一些warning，至于warning到底是真的有问题，还是可以忽略，则可以参考原文第5章，结合模型本身来分析。</li>
<li>运行test或train代码时可能会报以下错误，这是docker的运行环境的共享内存不够大导致的。</li>
</ul>
<blockquote>
<p>ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).</p>
</blockquote>
<p>可以通过给<code>docker run</code>命令增加<code>--shm-size 8G</code>配置选项（未验证正确性）或者替换<code>dataloader.py</code>中如下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">8</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<p>替换为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>运行test或train代码时可能会报以下错误，这是GPU显存不够导致的。</li>
</ul>
<blockquote>
<p>RuntimeError: CUDA out of memory. Tried to allocate 338.00 MiB (GPU 0; 4.00 GiB total capacity; 2.31 GiB already allocated; 0 bytes free; 2.42 GiB reserved in total by PyTorch)</p>
</blockquote>
<p>可以尝试将batch_size从64改成较小的值（16）来解决。</p>
<h2 id="结果">结果</h2>
<p>当执行量化感知重训之后，会将每个epoch的权重状态保存到checkpoint文件夹中，并会有以下信息打印：</p>
<blockquote>
<p>[Info] Test before quantization</p>
<p>[Info] Acc@1: 0.8366 Acc@5: 0.9747</p>
<p>…</p>
<p>[Info] Test after quantization</p>
<p>[Info] Acc@1: 0.8277 Acc@5: 0.9749</p>
<p>…</p>
<p>[Info] Save model at epoch 19 with accuracy (0.8327299058837301, 0.9748974173304369, 0.6288983184442324)</p>
<p>…</p>
<p>[Info] Save model at epoch 20 with accuracy (0.8322471639093154, 0.9758629012792662, 0.6324191087352901)</p>
</blockquote>
<p>可以看到量化重训后模型的精度相比量化模型是有所提升的。</p>
<!-- flag of hidden posts --></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">旭穹</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://no5-aaron-wu.github.io/2022/01/04/AI-Tools-1-AmbaPytorchQuant/">https://no5-aaron-wu.github.io/2022/01/04/AI-Tools-1-AmbaPytorchQuant/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://no5-aaron-wu.github.io" target="_blank">旭穹の陋室</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/Tool/">Tool</a><a class="post-meta__tags" href="/tags/Amba/">Amba</a><a class="post-meta__tags" href="/tags/Quantization/">Quantization</a></div><div class="post_share"><div class="social-share" data-image="/images/%E6%88%98%E5%8F%8CCG_%E8%A1%80%E6%88%98.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/images/wechatpay.png" target="_blank"><img class="post-qr-code-img" src="/images/wechatpay.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/images/alipay.png" target="_blank"><img class="post-qr-code-img" src="/images/alipay.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/01/29/AI-Algorithm-17-FastExp/" title="AI算法基础 [17]：快速浮点exp算法"><img class="cover" src="/images/%E6%88%98%E5%8F%8CCG_%E5%BF%83%E8%B1%A1%E8%BE%B9%E5%A2%83.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-29</div><div class="title">AI算法基础 [17]：快速浮点exp算法</div></div></a></div><div><a href="/2021/12/29/AI-Algorithm-16-GroupConv/" title="AI算法基础 [16]：分组网络GroupConv"><img class="cover" src="/images/%E6%88%98%E5%8F%8CCG_%E6%9A%82%E6%97%B6%E7%A6%BB%E5%88%AB.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-29</div><div class="title">AI算法基础 [16]：分组网络GroupConv</div></div></a></div><div><a href="/2021/12/29/AI-Algorithm-15-DeformConv/" title="AI算法基础 [15]：可形变卷积网络DeformConv"><img class="cover" src="/images/%E6%88%98%E5%8F%8CCG_%E8%B0%A2%E5%B9%95.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-29</div><div class="title">AI算法基础 [15]：可形变卷积网络DeformConv</div></div></a></div><div><a href="/2021/12/23/AI-Algorithm-14-GEMM-V2/" title="AI算法基础 [14]：GEMM进一步优化"><img class="cover" src="/images/%E6%88%98%E5%8F%8CCG_%E5%87%9D%E8%A7%86.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-23</div><div class="title">AI算法基础 [14]：GEMM进一步优化</div></div></a></div><div><a href="/2021/12/13/AI-Algorithm-13-Transformer/" title="AI算法基础 [13]：初探Transformer"><img class="cover" src="/images/%E6%88%98%E5%8F%8CCG_%E5%AF%B9%E5%B3%99.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-13</div><div class="title">AI算法基础 [13]：初探Transformer</div></div></a></div><div><a href="/2021/12/09/AI-Algorithm-12-GEMM/" title="AI算法基础 [12]：GEMM优化"><img class="cover" src="/images/%E6%88%98%E5%8F%8CCG_%E5%80%9A%E9%9D%A0.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-09</div><div class="title">AI算法基础 [12]：GEMM优化</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">旭穹</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">53</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/no5-aaron-wu"><i class="fab fa-github"></i><span>我的github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="mailto:no5aaron@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">安装</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#windows"><span class="toc-number">2.1.</span> <span class="toc-text">windows</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85pytorch"><span class="toc-number">2.1.1.</span> <span class="toc-text">安装pytorch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83"><span class="toc-number">2.1.2.</span> <span class="toc-text">配置环境</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#linux%EF%BC%88%E4%BC%AA%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">linux（伪）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E9%95%9C%E5%83%8F"><span class="toc-number">2.2.1.</span> <span class="toc-text">准备镜像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E5%8D%87%E7%BA%A7"><span class="toc-number">2.2.2.</span> <span class="toc-text">系统升级</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B8%85%E7%90%86%E9%95%9C%E5%83%8F%E8%87%AA%E5%B8%A6CUDA"><span class="toc-number">2.2.3.</span> <span class="toc-text">清理镜像自带CUDA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%87%E7%BA%A7WIN11"><span class="toc-number">2.2.4.</span> <span class="toc-text">升级WIN11</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85CUDA10-2"><span class="toc-number">2.2.5.</span> <span class="toc-text">安装CUDA10.2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85pytorch-2"><span class="toc-number">2.2.6.</span> <span class="toc-text">安装pytorch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8ONER2%E5%B7%A5%E7%A8%8B%E9%85%8D%E7%BD%AECVTOOLS%E7%8E%AF%E5%A2%83"><span class="toc-number">2.2.7.</span> <span class="toc-text">利用ONER2工程配置CVTOOLS环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E5%85%B6%E4%BB%96%E4%BE%9D%E8%B5%96"><span class="toc-number">2.2.8.</span> <span class="toc-text">安装其他依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E9%87%8F%E5%8C%96%E5%B7%A5%E5%85%B7"><span class="toc-number">2.2.9.</span> <span class="toc-text">安装量化工具</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96"><span class="toc-number">2.2.10.</span> <span class="toc-text">环境配置优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vscode-docker%EF%BC%88%E9%9D%9E%E5%BF%85%E9%A1%BB%EF%BC%89"><span class="toc-number">2.2.11.</span> <span class="toc-text">vscode+docker（非必须）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">示例数据下载</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">量化工具</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E4%BD%93%E6%B5%81%E7%A8%8B"><span class="toc-number">4.1.</span> <span class="toc-text">总体流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81"><span class="toc-number">4.2.</span> <span class="toc-text">修改模型代码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E5%85%B1%E4%BA%AB%E6%9D%83%E9%87%8D%E5%B1%82%E6%88%96%E9%87%8D%E7%94%A8%E5%B1%82"><span class="toc-number">4.2.1.</span> <span class="toc-text">修改共享权重层或重用层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E6%97%A0%E5%8F%82%E6%95%B0%E5%B1%82"><span class="toc-number">4.2.2.</span> <span class="toc-text">修改无参数层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97%E6%93%8D%E4%BD%9C"><span class="toc-number">4.2.3.</span> <span class="toc-text">修改张量运算操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9D%97%E5%88%97%E8%A1%A8"><span class="toc-number">4.2.4.</span> <span class="toc-text">修改模块列表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8BDecode%E5%B1%82"><span class="toc-number">4.2.5.</span> <span class="toc-text">修改模型Decode层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E5%B7%A5%E5%85%B7%E9%9B%86%E6%88%90"><span class="toc-number">4.3.</span> <span class="toc-text">量化工具集成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ONNX%E5%AF%BC%E5%87%BA%E4%BB%A3%E7%A0%81%E9%9B%86%E6%88%90"><span class="toc-number">4.3.1.</span> <span class="toc-text">ONNX导出代码集成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81%E9%9B%86%E6%88%90"><span class="toc-number">4.3.2.</span> <span class="toc-text">训练&#x2F;测试代码集成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E8%AE%AD%E7%BB%83%E8%B6%85%E5%8F%82"><span class="toc-number">4.3.3.</span> <span class="toc-text">重训练超参</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B%E9%87%8D%E8%AE%AD%E7%BB%83"><span class="toc-number">4.4.</span> <span class="toc-text">量化模型重训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E9%87%8D%E8%AE%AD%E6%B5%81%E7%A8%8B"><span class="toc-number">4.4.1.</span> <span class="toc-text">量化重训流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">4.5.</span> <span class="toc-text">量化函数使用方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-number">4.6.</span> <span class="toc-text">例子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C"><span class="toc-number">4.7.</span> <span class="toc-text">结果</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/08/31/EffectiveC-5-ReadNote05/" title="EffectiveC++-5-ReadNote05"><img src="/images/%E6%88%98%E5%8F%8CCG_%E8%AF%B4%E5%A3%B0%E5%86%8D%E8%A7%81.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="EffectiveC++-5-ReadNote05"/></a><div class="content"><a class="title" href="/2022/08/31/EffectiveC-5-ReadNote05/" title="EffectiveC++-5-ReadNote05">EffectiveC++-5-ReadNote05</a><time datetime="2022-08-31T11:12:16.000Z" title="发表于 2022-08-31 11:12:16">2022-08-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/14/cpp-modern-3-RValueReference/" title="现代C++ [3]: 右值引用、移动语义和完美转发"><img src="/images/pretty_derby_03.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="现代C++ [3]: 右值引用、移动语义和完美转发"/></a><div class="content"><a class="title" href="/2022/07/14/cpp-modern-3-RValueReference/" title="现代C++ [3]: 右值引用、移动语义和完美转发">现代C++ [3]: 右值引用、移动语义和完美转发</a><time datetime="2022-07-14T16:39:24.000Z" title="发表于 2022-07-14 16:39:24">2022-07-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/13/NLS-1-GetStart/" title="非线性最小二乘 [1]: GetStart"><img src="/images/pretty_derby_02.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="非线性最小二乘 [1]: GetStart"/></a><div class="content"><a class="title" href="/2022/07/13/NLS-1-GetStart/" title="非线性最小二乘 [1]: GetStart">非线性最小二乘 [1]: GetStart</a><time datetime="2022-07-13T14:45:59.000Z" title="发表于 2022-07-13 14:45:59">2022-07-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/08/EIGEN-1-GetStart/" title="Eigen[1]: GetStart"><img src="/images/pretty_derby_01.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Eigen[1]: GetStart"/></a><div class="content"><a class="title" href="/2022/07/08/EIGEN-1-GetStart/" title="Eigen[1]: GetStart">Eigen[1]: GetStart</a><time datetime="2022-07-08T11:11:20.000Z" title="发表于 2022-07-08 11:11:20">2022-07-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/20/EffectiveC-4-ReadNote04/" title="Effective C++ 读书笔记04"><img src="/images/zshz_01.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Effective C++ 读书笔记04"/></a><div class="content"><a class="title" href="/2022/06/20/EffectiveC-4-ReadNote04/" title="Effective C++ 读书笔记04">Effective C++ 读书笔记04</a><time datetime="2022-06-20T18:42:43.000Z" title="发表于 2022-06-20 18:42:43">2022-06-20</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/images/%E6%88%98%E5%8F%8CCG_%E8%A1%80%E6%88%98.png')"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 By 旭穹</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">谢临<a href="https://no5-aaron-wu.github.io/">陋室</a>, 欢迎留言！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script async="async">var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> {preloader.endLoading()})
setTimeout(function(){preloader.endLoading();}, 3000);</script></div><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '8238e4a92ed02fb5dcb8',
      clientSecret: '9c0dfb8fd7077aa7549fbda345a3b0c2cb781947',
      repo: 'no5-aaron-wu.github.io',
      owner: 'no5-aaron-wu',
      admin: ['no5-aaron-wu'],
      id: '56d416921f3cfba5e039b779e24e1121',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script><script src="/js/footer.js"></script><script src="/js/fishes.js"></script><script src="/live2d-widget/autoload.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>