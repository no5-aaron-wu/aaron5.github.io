<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>AI算法基础 [5]：MNN中的Winograd实现 | 旭穹の陋室</title><meta name="keywords" content="AI,Algorithm,MNN,Winograd"><meta name="author" content="旭穹"><meta name="copyright" content="旭穹"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="前言 主要针对CPU后端，基于&#x2F;source&#x2F;backend&#x2F;cpu&#x2F;compute&#x2F;ConvolutionWinograd.cpp源码展开。 部分章节以输入大小：1 x 8 x 224 x 224，权重大小: 16 x 8 x 3 x 3, 输出1 x 16 x 222 x 222 为例进行辅助说明。 MNN卷积相关运算统一使用CAFFE_C4格式，即MNN自创的NC4HW4格式，具体排布介绍：">
<meta property="og:type" content="article">
<meta property="og:title" content="AI算法基础 [5]：MNN中的Winograd实现">
<meta property="og:url" content="https://no5-aaron-wu.github.io/2021/11/16/AI-Algorithm-5-WinogradInMnn/index.html">
<meta property="og:site_name" content="旭穹の陋室">
<meta property="og:description" content="前言 主要针对CPU后端，基于&#x2F;source&#x2F;backend&#x2F;cpu&#x2F;compute&#x2F;ConvolutionWinograd.cpp源码展开。 部分章节以输入大小：1 x 8 x 224 x 224，权重大小: 16 x 8 x 3 x 3, 输出1 x 16 x 222 x 222 为例进行辅助说明。 MNN卷积相关运算统一使用CAFFE_C4格式，即MNN自创的NC4HW4格式，具体排布介绍：">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://no5-aaron-wu.github.io/images/%E6%88%98%E5%8F%8CCG_%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%97.png">
<meta property="article:published_time" content="2021-11-16T17:30:08.000Z">
<meta property="article:modified_time" content="2021-11-30T07:35:09.177Z">
<meta property="article:author" content="旭穹">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Algorithm">
<meta property="article:tag" content="MNN">
<meta property="article:tag" content="Winograd">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://no5-aaron-wu.github.io/images/%E6%88%98%E5%8F%8CCG_%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%97.png"><link rel="shortcut icon" href="/images/favicon-128x128.ico"><link rel="canonical" href="https://no5-aaron-wu.github.io/2021/11/16/AI-Algorithm-5-WinogradInMnn/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="zlQaq8XLp3Oj3DgDnpkA0mCzGbGvflp_U-vZVql_E-E"/><meta name="msvalidate.01" content="EAE2F4C7507064F526B5924DA5DCD005"/><meta name="baidu-site-verification" content="code-XsSJQ4q5sT"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-6DDC82R7DV"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6DDC82R7DV');
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#009f6a","bgDark":"#121212","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'AI算法基础 [5]：MNN中的Winograd实现',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-11-30 07:35:09'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mouse.css"><style>#article-container.post-content h1:before, h2:before, h3:before, h4:before, h5:before, h6:before { -webkit-animation: avatar_turn_around 1s linear infinite; -moz-animation: avatar_turn_around 1s linear infinite; -o-animation: avatar_turn_around 1s linear infinite; -ms-animation: avatar_turn_around 1s linear infinite; animation: avatar_turn_around 1s linear infinite; }</style><link rel="preconnect" href="https://fonts.gstatic.com"><link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@1,300&display=swap" rel="stylesheet"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="wizard-scene"><div class="wizard-objects"><div class="wizard-square"></div><div class="wizard-circle"></div><div class="wizard-triangle"></div></div><div class="wizard"><div class="wizard-body"></div><div class="wizard-right-arm"><div class="wizard-right-hand"></div></div><div class="wizard-left-arm"><div class="wizard-left-hand"></div></div><div class="wizard-head"><div class="wizard-beard"></div><div class="wizard-face"><div class="wizard-adds"></div></div><div class="wizard-hat"><div class="wizard-hat-of-the-hat"></div><div class="wizard-four-point-star --first"></div><div class="wizard-four-point-star --second"></div><div class="wizard-four-point-star --third"></div></div></div></div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">15</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/%E6%88%98%E5%8F%8CCG_%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%97.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">旭穹の陋室</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">AI算法基础 [5]：MNN中的Winograd实现</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-11-16T17:30:08.000Z" title="发表于 2021-11-16 17:30:08">2021-11-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-11-30T07:35:09.177Z" title="更新于 2021-11-30 07:35:09">2021-11-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="AI算法基础 [5]：MNN中的Winograd实现"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>前言</h1>
<p>主要针对CPU后端，基于<mnnrootdir>/source/backend/cpu/compute/ConvolutionWinograd.cpp源码展开。</p>
<p>部分章节以输入大小：<code>1 x 8 x 224 x 224</code>，权重大小: <code>16 x 8 x 3 x 3</code>, 输出<code>1 x 16 x 222 x 222</code> 为例进行辅助说明。</p>
<p>MNN卷积相关运算统一使用CAFFE_C4格式，即MNN自创的NC4HW4格式，具体排布介绍：<a href="https://no5-aaron-wu.github.io/2021/11/14/AI-Algorithm-2-NC4HW4/">NC4HW4数据排布</a></p>
<h1>Winograd适用条件</h1>
<p><code>canUseWinograd</code>函数，具体代码如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ConvolutionWinograd::canUseWinograd</span><span class="params">(<span class="keyword">const</span> Convolution2DCommon *common)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (common-&gt;<span class="built_in">kernelY</span>() != common-&gt;<span class="built_in">kernelX</span>() || common-&gt;<span class="built_in">kernelY</span>() &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (common-&gt;<span class="built_in">dilateX</span>() != <span class="number">1</span> || common-&gt;<span class="built_in">dilateY</span>() != <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (common-&gt;<span class="built_in">strideX</span>() != <span class="number">1</span> || common-&gt;<span class="built_in">strideY</span>() != <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总结出来就是:</p>
<ul>
<li>
<ol>
<li>必须是<code>kernelX = kernelY</code>的卷积核，并且卷积核尺寸大于<code>1x1</code>。<code>1x1</code>的卷积核MNN使用的是<strong>Convolution1x1Strassen</strong>实现，暂且按下不表。</li>
</ol>
</li>
<li>
<ol start="2">
<li>不适用于<code>dilateConv</code>。</li>
</ol>
</li>
<li>
<ol start="3">
<li>不适用于<code>stride != 1</code>的conv。</li>
</ol>
</li>
</ul>
<h1>选取最佳的Unit</h1>
<p><code>bestWinogradUnit</code>函数，具体代码如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">ConvolutionWinograd::bestWinogradUnit</span><span class="params">(<span class="keyword">const</span> Convolution2DCommon *common, <span class="keyword">const</span> Tensor *inputTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          <span class="keyword">const</span> Tensor *outputTensor, <span class="keyword">int</span> threadNumber, Backend* b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> core = <span class="keyword">static_cast</span>&lt;CPUBackend*&gt;(b)-&gt;<span class="built_in">functions</span>();</span><br><span class="line">    <span class="keyword">int</span> ow      = outputTensor-&gt;<span class="built_in">width</span>();</span><br><span class="line">    <span class="keyword">int</span> oh      = outputTensor-&gt;<span class="built_in">height</span>();</span><br><span class="line">    <span class="keyword">int</span> oc      = outputTensor-&gt;<span class="built_in">channel</span>();</span><br><span class="line">    <span class="keyword">int</span> ePack, hPack, lPack;</span><br><span class="line">    core-&gt;<span class="built_in">MNNGetMatMulPackMode</span>(&amp;ePack, &amp;lPack, &amp;hPack);</span><br><span class="line">    <span class="keyword">int</span> unit2   = <span class="built_in">UP_DIV</span>(ow * oh, ePack * threadNumber);</span><br><span class="line">    <span class="keyword">int</span> maxUnit = (<span class="keyword">int</span>)::<span class="built_in">sqrtf</span>((<span class="keyword">float</span>)unit2);</span><br><span class="line">    maxUnit     = std::<span class="built_in">min</span>(maxUnit, CONVOLUTION_WINOGRAD_MAX_UNIT);</span><br><span class="line">    maxUnit     = std::<span class="built_in">max</span>(maxUnit, CONVOLUTION_WINOGRAD_MIN_UNIT);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> ic           = inputTensor-&gt;<span class="built_in">channel</span>();</span><br><span class="line">    <span class="keyword">auto</span> kernelSize  = common-&gt;<span class="built_in">kernelY</span>();</span><br><span class="line">    <span class="keyword">int</span> unit         = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">float</span> maxRate    = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">float</span> originCost = (<span class="keyword">float</span>)ow * oh * (<span class="keyword">float</span>)ic * oc * kernelSize * kernelSize;</span><br><span class="line">    std::set&lt;<span class="keyword">int</span>&gt; supportSu&#123;<span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> u = CONVOLUTION_WINOGRAD_MIN_UNIT; u &lt;= maxUnit; ++u) &#123;</span><br><span class="line">        <span class="keyword">auto</span> sui = u + kernelSize - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">auto</span> su = (<span class="keyword">float</span>)sui;</span><br><span class="line">        <span class="keyword">if</span> (supportSu.<span class="built_in">find</span>(sui) == supportSu.<span class="built_in">end</span>()) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (<span class="literal">nullptr</span> == core-&gt;<span class="built_in">chooseWinoDestTransform</span>((<span class="keyword">int</span>)su, u)) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*Let F(6,3) be choosed when it can speed up from F(2,3) than 0.6*/</span></span><br><span class="line">        <span class="keyword">float</span> penalty = (su * su) / (<span class="keyword">float</span>)(kernelSize * kernelSize) * <span class="number">0.12f</span>;</span><br><span class="line">        <span class="keyword">float</span> winogradCost =</span><br><span class="line">            (<span class="number">2</span> * su * su * ic + su * su * ic * oc + (su + u) * u * oc) * (<span class="built_in">UP_DIV</span>(ow, u) * <span class="built_in">UP_DIV</span>(oh, u));</span><br><span class="line">        <span class="keyword">float</span> reduceRate = originCost / winogradCost - penalty;</span><br><span class="line">        <span class="comment">// MNN_PRINT(&quot;ow=%d, oh=%d, %f, %f, winograd unit:%d\n&quot;, ow, oh, winogradCost, reduceRate, u);</span></span><br><span class="line">        <span class="keyword">if</span> (reduceRate &gt; maxRate) &#123;</span><br><span class="line">            maxRate = reduceRate;</span><br><span class="line">            unit    = u;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (maxRate &lt; <span class="number">1.0f</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> unit;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>
<ol>
<li>经确认，MNN中计算Cost并不是使用<code>乘加次数</code>的度量方式，而是<code>访存量</code>的度量方式，即一次winograd计算中对内存读取的次数；</li>
</ol>
</li>
<li>
<ol start="2">
<li>上述代码的大致逻辑就是遍历<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mi>n</mi><mi>i</mi><mi>t</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>2</mn><mo separator="true">,</mo><mn>8</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">unit\in[2,8]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6986em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">u</span><span class="mord mathnormal">ni</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">8</span><span class="mclose">]</span></span></span></span>，计算普通卷积<code>originCost</code>和<code>winogradCost</code>的比值，寻找比值的最大值，来确定最佳unit；</li>
</ol>
</li>
<li>
<ol start="3">
<li>但是在计算输出转换是<code>(su + u) * u * oc</code>，但我觉得应该是<code>(su + u) * su * oc</code>才对；</li>
</ol>
</li>
<li>
<ol start="4">
<li>unit选取其实是个权衡值，在内存访问次数和乘法次数之间存在一定权衡，unit小的乘法次数少，但是内存访问次数多，反之反之;</li>
</ol>
</li>
<li>
<ol start="5">
<li><code>penalty</code>是个惩罚值，具体作用可以看这个<a target="_blank" rel="noopener" href="https://github.com/alibaba/MNN/issues/1340">issue</a>。</li>
</ol>
</li>
</ul>
<h1>生成变换矩阵</h1>
<p>通过<code>WinogradGenerater</code>类实现，主要根据<code>unit</code>和<code>kernelSize</code>大小计算三个转换矩阵:<code>G</code>，<code>A</code>，<code>B</code>，根据<strong>中国余数定理</strong>求解同余方程组，获取变换矩阵。暂且按下不表</p>
<h1>权重变换</h1>
<blockquote>
<p>以unit=6为例</p>
</blockquote>
<p>原始权重Tensor大小<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Red"><mn>16</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Red}16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Red;">16</span></span></span></span> × <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Blue"><mn>8</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Blue}8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Blue;">8</span></span></span></span> × <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Green"><mn>3</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Green}3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Green;">3</span></span></span></span> × <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Green"><mn>3</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Green}3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Green;">3</span></span></span></span>，每个3×3 kernel变换后Tensor大小应为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Red"><mn>16</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Red}16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Red;">16</span></span></span></span> × <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Blue"><mn>8</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Blue}8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Blue;">8</span></span></span></span> × <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Green"><mn>8</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Green}8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Green;">8</span></span></span></span> × <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Green"><mn>8</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Green}8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Green;">8</span></span></span></span>，经过<code>NC4HW4</code>重排后的Tensor大小 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Green"><mn>64</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Green}64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Green;">64</span></span></span></span> × <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Red"><mn>4</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Red}4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Red;">4</span></span></span></span> × <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Blue"><mn>8</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Blue}8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Blue;">8</span></span></span></span> × <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Blue"><mn>1</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Blue}1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Blue;">1</span></span></span></span> × <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Red"><mn>4</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Red}4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Red;">4</span></span></span></span>。</p>
<p>重排后每个维度含义如下：</p>
<ul>
<li>
<ol>
<li>变换矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">G</span></span></span></span>大小为 <code>8 x 3</code>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>G</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">G^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>为 <code>3 x 8</code>，根据变换公式，每个<code>3 x 3</code>卷积核变换后的大小为 <code>8 x 8 = 64</code>，将这64个值拆分到64个batch（<code>N</code>维度）；</li>
</ol>
</li>
<li>
<ol start="2">
<li>卷积核个数16按照<code>C4</code>进行拆分，每个<code>C4 pack</code>排布到<code>W</code>维度，<code>C</code>维度为4；</li>
</ol>
</li>
<li>
<ol start="3">
<li>单个卷积核通道数8排布到<code>H</code>维度；</li>
</ol>
</li>
</ul>
<p>这样做的目的是便于后面<code>MatMul</code>时，指令集可以一次性对<strong>同一个输入</strong>的<strong>4个不同卷积核</strong>的<strong>单个通道</strong>进行乘法计算。在H方向累加就可以实现<strong>同一个输入</strong>的<strong>不同卷积核</strong>的卷积结果输出。示意图如下：</p>
<p><img src="1.png" alt="1.png"></p>
<p>具体代码如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">WinogradGenerater::transformWeight</span><span class="params">(<span class="keyword">const</span> Tensor* weightDest, <span class="keyword">const</span> Tensor* source, <span class="keyword">bool</span> ciFirst)</span> </span>&#123;</span><br><span class="line">    <span class="function">std::shared_ptr&lt;Tensor&gt; <span class="title">GT</span><span class="params">(Math::Matrix::create(mG-&gt;length(<span class="number">0</span>), mG-&gt;length(<span class="number">1</span>)))</span></span>;</span><br><span class="line">    Math::Matrix::<span class="built_in">transpose</span>(GT.<span class="built_in">get</span>(), mG.<span class="built_in">get</span>());</span><br><span class="line">    <span class="keyword">int</span> ci          = source-&gt;<span class="built_in">length</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">int</span> co          = source-&gt;<span class="built_in">length</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">int</span> kernelCount = source-&gt;<span class="built_in">length</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">int</span> unitCi      = weightDest-&gt;<span class="built_in">length</span>(<span class="number">3</span>);</span><br><span class="line">    <span class="keyword">int</span> unitCo      = weightDest-&gt;<span class="built_in">length</span>(<span class="number">4</span>);</span><br><span class="line">    <span class="keyword">auto</span> alpha      = mB-&gt;<span class="built_in">length</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ci % unitCi != <span class="number">0</span> || co % unitCo != <span class="number">0</span>) &#123;</span><br><span class="line">        ::<span class="built_in">memset</span>(weightDest-&gt;host&lt;<span class="keyword">float</span>&gt;(), <span class="number">0</span>, weightDest-&gt;<span class="built_in">size</span>());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">std::shared_ptr&lt;Tensor&gt; <span class="title">M</span><span class="params">(Math::Matrix::create(kernelCount, alpha))</span></span>;</span><br><span class="line">    <span class="function">std::shared_ptr&lt;Tensor&gt; <span class="title">K</span><span class="params">(Math::Matrix::createShape(kernelCount, kernelCount))</span></span>;</span><br><span class="line">    <span class="function">std::shared_ptr&lt;Tensor&gt; <span class="title">K_Transform</span><span class="params">(Math::Matrix::create(alpha, alpha))</span></span>;</span><br><span class="line">    <span class="keyword">auto</span> weightPtr      = source-&gt;host&lt;<span class="keyword">float</span>&gt;();</span><br><span class="line">    <span class="keyword">auto</span> KTransformData = K_Transform-&gt;host&lt;<span class="keyword">float</span>&gt;();</span><br><span class="line">    <span class="keyword">int</span> lCi = unitCo;</span><br><span class="line">    <span class="keyword">int</span> lCo = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (ciFirst) &#123;</span><br><span class="line">        lCi = <span class="number">1</span>;</span><br><span class="line">        lCo = unitCi;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> oz = <span class="number">0</span>; oz &lt; co; ++oz) &#123;</span><br><span class="line">        <span class="keyword">auto</span> srcOz = weightPtr + oz * ci * kernelCount * kernelCount;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> ozC4 = oz / unitCo;</span><br><span class="line">        <span class="keyword">int</span> mx   = oz % unitCo;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">auto</span> dstOz = weightDest-&gt;host&lt;<span class="keyword">float</span>&gt;() + weightDest-&gt;<span class="built_in">stride</span>(<span class="number">1</span>) * ozC4 + mx * lCo;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> sz = <span class="number">0</span>; sz &lt; ci; ++sz) &#123;</span><br><span class="line">            <span class="keyword">int</span> szC4         = sz / unitCi;</span><br><span class="line">            <span class="keyword">int</span> my           = sz % unitCi;</span><br><span class="line">            <span class="keyword">auto</span> srcSz       = srcOz + kernelCount * kernelCount * sz;</span><br><span class="line">            K-&gt;<span class="built_in">buffer</span>().host = (<span class="keyword">uint8_t</span>*)srcSz;</span><br><span class="line">            <span class="comment">// M = G * K</span></span><br><span class="line">            Math::Matrix::<span class="built_in">multi</span>(M.<span class="built_in">get</span>(), mG.<span class="built_in">get</span>(), K.<span class="built_in">get</span>());</span><br><span class="line">            <span class="comment">// K_Transform = M*GT</span></span><br><span class="line">            Math::Matrix::<span class="built_in">multi</span>(K_Transform.<span class="built_in">get</span>(), M.<span class="built_in">get</span>(), GT.<span class="built_in">get</span>());</span><br><span class="line"></span><br><span class="line">            <span class="keyword">auto</span> dstSz = dstOz + szC4 * weightDest-&gt;<span class="built_in">stride</span>(<span class="number">2</span>) + my * lCi;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; alpha * alpha; ++i) &#123;</span><br><span class="line">                *(dstSz + i * weightDest-&gt;<span class="built_in">stride</span>(<span class="number">0</span>)) = KTransformData[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1>分块处理</h1>
<h2 id="常用术语">常用术语</h2>
<ul>
<li>
<ol>
<li><code>unit</code>：Winograd输出FeatureMap的分块大小。</li>
</ol>
</li>
</ul>
<blockquote>
<p>假设卷积输出的Tensor大小为: <code>1 x 16 x 222 x 222</code>，<code>kernel size = 3</code>，<code>unit = 6</code>，则输出按照<code>6 x 6</code>大小分块输出，输入中就使用<code>8 x 8</code>大小分块。简单说就是利用输入的<code>8 x 8</code>块计算输出的<code>6 x 6</code>块。那么<code>222 x 222</code> 就可以分为 <code>37 x 37</code>个块大小。</p>
</blockquote>
<ul>
<li>
<ol start="2">
<li><code>epack</code>：分组单位，固定值，对于不同的后端设置为不同的值。获取方式如下：</li>
</ol>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> ePack, lPack, hPack;</span><br><span class="line">core-&gt;<span class="built_in">MNNGetMatMulPackMode</span>(&amp;ePack, &amp;lPack, &amp;hPack);</span><br></pre></td></tr></table></figure>
<ul>
<li>
<ol start="3">
<li><code>tile</code>： 按照uint进行分块后，再对所有块进行分组的个数。</li>
</ol>
</li>
</ul>
<blockquote>
<p>如上面<code>37 x 37</code>个块，如果<code>epack = 24</code>，则<code>tile =（37 x 37 + 24 - 1）/ 24 = 58</code>。保证所有块都能处理到，因此采用向上取整。</p>
</blockquote>
<h2 id="分块原理">分块原理</h2>
<p>分块原理如下图所示：</p>
<p><img src="2.png" alt="2.png"></p>
<p>分块流程确定后，实操过程并不是把所有输入<code>tile</code>都处理完，才执行后面的乘加及输出变换，这样的内存开销太大了，不可取。<br>
MNN采用的是以<code>tile</code>为单位进行处理，每个<code>tile</code>进行输入变换、乘加、输出变换。将结果逐个写入到输出Tensor中。另外，由于<code>tile</code>之间是完全独立的，也方便使用多线程处理。</p>
<p><code>1 x 2 x 224 x 224 x 4</code>的输入，按照<code>tile</code>划分后变形为(NCHW): <code>1x 2 x 58</code> <code>x</code> <strong><code>tile</code></strong>，这其中的2实际是输入通道8经过<code>C4 Pack</code>后的值，在卷积过程中是要一起累加到输出结果的。因此实际计算的时候，按照一次处理tile的两个通道进行计算。一共循环处理58次。</p>
<h1>buffer分配</h1>
<blockquote>
<p>延续前面例子的尺寸情况</p>
</blockquote>
<p>具体代码如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">mTempBuffer.<span class="built_in">reset</span>(Tensor::createDevice&lt;<span class="keyword">uint8_t</span>&gt;(&#123;threadNumber, ePack, ic4 + oc4, pack * alpha2, bytes&#125;));</span><br><span class="line">mTransformMidBuffer.<span class="built_in">reset</span>(Tensor::createDevice&lt;<span class="keyword">uint8_t</span>&gt;(&#123;threadNumber, (<span class="number">1</span> + ic4 * ePack), alpha2, pack, bytes&#125;)); <span class="comment">// 1 means original small buffer of alpha2 * pack.</span></span><br><span class="line">mGemmMidBuffer.<span class="built_in">reset</span>(Tensor::createDevice&lt;<span class="keyword">uint8_t</span>&gt;(&#123;threadNumber, alpha, ePack * <span class="built_in">UP_DIV</span>(srcCount, pack) * pack, bytes&#125;));</span><br></pre></td></tr></table></figure>
<p>参数的具体含义如下：</p>
<ul>
<li>
<ol>
<li><code>ePack = 24</code>表示一个<code>tile</code>有24个块。</li>
</ol>
</li>
<li>
<ol start="2">
<li><code>ic4 = 2</code>，<code>oc4 = 4</code>，分别代表<code>CAFFE_C4</code>格式下的输入，输出通道数。</li>
</ol>
</li>
<li>
<ol start="3">
<li><code>pack = 4</code>表示4个通道为一组pack到一起。</li>
</ol>
</li>
<li>
<ol start="4">
<li><code>alpha2 = 8 x 8 = 64</code>，表示单个输入块的数据个数。</li>
</ol>
</li>
<li>
<ol start="5">
<li><code>bytes=4</code>，处理float类型，4个字节。</li>
</ol>
</li>
</ul>
<p>3个buffer的情况如下：</p>
<ul>
<li>
<ol>
<li><code>mTempBuffer</code>：单线程为例，分配的大小为 <code>1 x 24 x (2 + 4) x (4 x 64) x 4</code>bytes。其中，
<ul>
<li><code>1 x 24 x 2 x (4 x 64) x 4</code>bytes用于存储输入变换后的数据。</li>
<li><code>1 x 24 x 4 x (4 x 64) x 4</code>bytes用于存储输出变换后的数据。</li>
</ul>
</li>
</ol>
</li>
<li>
<ol start="2">
<li><code>mTransformMidBuffer</code>：单线程为例，分配的大小为 <code>1 x 2 x 64 x 4 x 4</code>bytes，用于存储单个输入块的两个通道变换后的数据。</li>
</ol>
</li>
<li>
<ol start="3">
<li><code>mGemmMidBuffer</code>：单线程为例，分配的大小为<code>1 x (24 x 8) x 4</code>bytes，单个乘加模块需要的输入数据，即：<code>24 x 8</code>个float，至于这<code>24 x 8</code>是以怎样的形式取得，下文有阐述。</li>
</ol>
</li>
</ul>
<p>通俗来说就是针对每个输出通道，一次处理24个输入的8通道数据。乘加后得到24个输出。循环64次，得到单个输出通道中一个<code>tile</code>的所有输出（未做输出变换前）： <code>24 x 8 x 8</code>。</p>
<h1>输入变换</h1>
<p>由上面一节可知，一次处理2个通道的tile，即: <code>2 x 24 x 8 x 8 x4</code>的float数据量。</p>
<p>2个通道的块处理的核心代码部分见这里，外面会循环24次：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> z = <span class="number">0</span>; z &lt; ic_4; ++z) &#123;</span><br><span class="line">    <span class="keyword">auto</span> srcZ = srcStart + z * sourceZStep * bytes;</span><br><span class="line">    <span class="comment">// Transform</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; srcUnit; ++i) &#123;</span><br><span class="line">        <span class="keyword">auto</span> srcFloatPtr = (<span class="keyword">const</span> <span class="keyword">float</span>*)(srcZ + i * iw * pack * bytes);</span><br><span class="line">        <span class="keyword">auto</span> dstFloatPtr = (<span class="keyword">float</span>*)(midBuffer1 + i * pack * bytes);</span><br><span class="line">        <span class="built_in">mSourceTransform</span>(srcFloatPtr, dstFloatPtr, pack, pack * srcUnit);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">auto</span> dstZ = dst_x + z * dstZStep * bytes;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; srcUnit; ++i) &#123;</span><br><span class="line">        <span class="keyword">auto</span> srcFloatPtr = (<span class="keyword">const</span> <span class="keyword">float</span>*)(midBuffer1 + i * srcUnit * pack * bytes);</span><br><span class="line">        <span class="keyword">auto</span> dstFloatPtr = (<span class="keyword">float</span>*)(dstZ + i * unitStep * bytes);</span><br><span class="line">        <span class="built_in">mSourceTransform</span>(srcFloatPtr, dstFloatPtr, pack,</span><br><span class="line">                         unitStep * srcUnit);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>mSourceTransform</code>函数基于<code>unit</code>而有不同的实现，这里贴出<code>8 x 8</code>的实现：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LOAD8                                     \</span></span><br><span class="line"><span class="meta">    Vec4 s0 = Vec4::load(srcBlock + 0 * srcStep); \</span></span><br><span class="line"><span class="meta">    Vec4 s1 = Vec4::load(srcBlock + 1 * srcStep); \</span></span><br><span class="line"><span class="meta">    Vec4 s2 = Vec4::load(srcBlock + 2 * srcStep); \</span></span><br><span class="line"><span class="meta">    Vec4 s3 = Vec4::load(srcBlock + 3 * srcStep); \</span></span><br><span class="line"><span class="meta">    Vec4 s4 = Vec4::load(srcBlock + 4 * srcStep); \</span></span><br><span class="line"><span class="meta">    Vec4 s5 = Vec4::load(srcBlock + 5 * srcStep); \</span></span><br><span class="line"><span class="meta">    Vec4 s6 = Vec4::load(srcBlock + 6 * srcStep); \</span></span><br><span class="line"><span class="meta">    Vec4 s7 = Vec4::load(srcBlock + 7 * srcStep);</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> _sourceTransformUnit8x8(<span class="keyword">const</span> <span class="keyword">float</span>* srcBlock, <span class="keyword">float</span>* dstStart, <span class="keyword">size_t</span> srcStep, <span class="keyword">size_t</span> dstStep) &#123;</span><br><span class="line">    LOAD8;</span><br><span class="line">    Vec4 m0 = s0 * <span class="number">36.f</span> - s2 * <span class="number">49.f</span> + s4 * <span class="number">14.f</span> - s6;</span><br><span class="line">    Vec4 m1 = (s1 + s2) * <span class="number">36.f</span> - (s3 + s4) * <span class="number">13.f</span> + (s5 + s6);</span><br><span class="line">    Vec4 m2 = (s2 - s1) * <span class="number">36.f</span> + (s3 - s4) * <span class="number">13.f</span> + (s6 - s5);</span><br><span class="line">    Vec4 m3 = s1 * <span class="number">18.f</span> + s2 * <span class="number">9.f</span> - s3 * <span class="number">20.f</span> - s4 * <span class="number">10.f</span> + s5 * <span class="number">2.f</span> + s6;</span><br><span class="line">    Vec4 m4 = s2 * <span class="number">9.f</span> - s1 * <span class="number">18.f</span> + s3 * <span class="number">20.f</span> - s4 * <span class="number">10.f</span> - s5 * <span class="number">2.f</span> + s6;</span><br><span class="line">    Vec4 m5 = s1 * <span class="number">12.f</span> + s2 * <span class="number">4.f</span> - s3 * <span class="number">15.f</span> - s4 * <span class="number">5.f</span> + s5 * <span class="number">3.f</span> + s6;</span><br><span class="line">    Vec4 m6 = s2 * <span class="number">4.f</span> - s1 * <span class="number">12.f</span> + s3 * <span class="number">15.f</span> - s4 * <span class="number">5.f</span> - s5 * <span class="number">3.f</span> + s6;</span><br><span class="line">    Vec4 m7 = s3 * <span class="number">49.f</span> - s1 * <span class="number">36.f</span> - s5 * <span class="number">14.f</span> + s7;</span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">0</span> * dstStep, m0);</span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">1</span> * dstStep, m1);</span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">2</span> * dstStep, m2);</span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">3</span> * dstStep, m3);</span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">4</span> * dstStep, m4);</span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">5</span> * dstStep, m5);</span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">6</span> * dstStep, m6);</span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">7</span> * dstStep, m7);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>处理一行输入数据，调用了两次<code>mSouceTransform</code>，其中每次<code>mSouceTransform</code>里面做的是:</p>
<ul>
<li>
<ol>
<li><code>LOAD8</code>：指令集连续加载一个块中一行的8个<code>C4 Pack</code>的数据（可以视为一个行向量），放到8个128bit的寄存器<code>s0 - s7</code>中；</li>
</ol>
</li>
<li>
<ol start="2">
<li>右乘变换矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>，得到相乘结果 <code>m0 - m7</code>；</li>
</ol>
</li>
<li>
<ol start="3">
<li><code>m0-m7</code>转置（<code>+ n * dstStep</code>按列存放）后，输出到临时的<code>mTransformMidBuffer</code>中。</li>
</ol>
</li>
</ul>
<p>其实MNN对原始变换公式做了等价调整，这样调整能够便于代码复用，不用同时保留矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>及矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>B</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">B^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>B</mi><mi>T</mi></msup><mi>D</mi><mi>B</mi><mo>=</mo><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi>D</mi><mi>B</mi><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mi>B</mi><msup><mo stretchy="false">)</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">B^TDB=((DB)^TB)^T
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mopen">((</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>一行处理完毕后，循环8次，将8行数据全部处理完，即完成了<code>8x8</code>一个块的变换。然后循环2次，把一个块(block)的2个<code>C4 Pack</code>通道也处理完毕，保存到<code>mTempBuffer</code>中，保存排布是这样的：</p>
<p><img src="3.png" alt="3.png"></p>
<h1>MatMul + Add 乘加计算</h1>
<p>单个tile的输入转换完毕后，可以准备乘加计算了。MNN源码如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; srcUnit2; ++i) &#123;</span><br><span class="line">    <span class="keyword">auto</span> srcTemp = (<span class="keyword">const</span> <span class="keyword">float</span>*)(_srcOrigin + i * ic_4 * pack * xC * bytes);</span><br><span class="line">    <span class="keyword">auto</span> _dstFloatPtr = (<span class="keyword">float</span>*)(_dstOrigin + i * dc_4 * pack * xC * bytes);</span><br><span class="line">    <span class="keyword">auto</span> _weightFloatPtr = (<span class="keyword">const</span> <span class="keyword">float</span>*)(weight + i * mResource-&gt;mWeight-&gt;<span class="built_in">stride</span>(<span class="number">0</span>));</span><br><span class="line">    core-&gt;<span class="built_in">MNNPackC4ForMatMul_A</span>((<span class="keyword">float</span>*)gemmBuffer, &amp;srcTemp, info, el);</span><br><span class="line">    core-&gt;<span class="built_in">MNNPackedMatMul</span>(_dstFloatPtr, (<span class="keyword">float</span>*)gemmBuffer, _weightFloatPtr, parameters.<span class="built_in">data</span>(), <span class="literal">nullptr</span>, <span class="literal">nullptr</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>根据上面的转换图可知，<code>tile</code>中的数据排布成64行，每行24个<code>C4 Pack</code>，一共2个通道。</p>
<p>首先最外层for循环<code>srcUint2 = 8 x 8 = 64</code>，即逐行遍历。每行的处理包括<code>MNNPackC4ForMatMul_A</code> 及 <code>MNNPackedMatMul</code>两个步骤：</p>
<h2 id="MNNPackC4ForMatMul-A">MNNPackC4ForMatMul_A</h2>
<blockquote>
<p>此处代码为MNN release_1.1.7版本（为了便于跟上述逻辑连贯），后续版本AVX加速代码有更新(AVX有256位寄存器，可以从<code>C4 Pack</code> 升到<code>C8 pack</code> )</p>
</blockquote>
<p><code>_AVX_MNNPackC4ForMatMul_A</code>函数核心代码（AVX指令集加速版本）如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAIN_COMPUTE                        \</span></span><br><span class="line"><span class="meta">    auto s00 = _mm_loadu_ps(srcX + 0 * pOffset);  \</span></span><br><span class="line"><span class="meta">    auto s01 = _mm_loadu_ps(srcX + 1 * pOffset);  \</span></span><br><span class="line"><span class="meta">    auto s02 = _mm_loadu_ps(srcX + 2 * pOffset);  \</span></span><br><span class="line"><span class="meta">    auto s03 = _mm_loadu_ps(srcX + 3 * pOffset);  \</span></span><br><span class="line"><span class="meta">    auto s10 = _mm_loadu_ps(srcX + 4 * pOffset);  \</span></span><br><span class="line"><span class="meta">    auto s11 = _mm_loadu_ps(srcX + 5 * pOffset);  \</span></span><br><span class="line"><span class="meta">    auto s12 = _mm_loadu_ps(srcX + 6 * pOffset);  \</span></span><br><span class="line"><span class="meta">    auto s13 = _mm_loadu_ps(srcX + 7 * pOffset);  \</span></span><br><span class="line"><span class="meta">    auto s20 = _mm_loadu_ps(srcX + 8 * pOffset);  \</span></span><br><span class="line"><span class="meta">    auto s21 = _mm_loadu_ps(srcX + 9 * pOffset);  \</span></span><br><span class="line"><span class="meta">    auto s22 = _mm_loadu_ps(srcX + 10 * pOffset); \</span></span><br><span class="line"><span class="meta">    auto s23 = _mm_loadu_ps(srcX + 11 * pOffset); \</span></span><br><span class="line"><span class="meta">    auto s30 = _mm_loadu_ps(srcX + 12 * pOffset); \</span></span><br><span class="line"><span class="meta">    auto s31 = _mm_loadu_ps(srcX + 13 * pOffset); \</span></span><br><span class="line"><span class="meta">    auto s32 = _mm_loadu_ps(srcX + 14 * pOffset); \</span></span><br><span class="line"><span class="meta">    auto s33 = _mm_loadu_ps(srcX + 15 * pOffset); \</span></span><br><span class="line"><span class="meta">    auto s40 = _mm_loadu_ps(srcX + 16 * pOffset); \</span></span><br><span class="line"><span class="meta">    auto s41 = _mm_loadu_ps(srcX + 17 * pOffset); \</span></span><br><span class="line"><span class="meta">    auto s42 = _mm_loadu_ps(srcX + 18 * pOffset); \</span></span><br><span class="line"><span class="meta">    auto s43 = _mm_loadu_ps(srcX + 19 * pOffset); \</span></span><br><span class="line"><span class="meta">    auto s50 = _mm_loadu_ps(srcX + 20 * pOffset); \</span></span><br><span class="line"><span class="meta">    auto s51 = _mm_loadu_ps(srcX + 21 * pOffset); \</span></span><br><span class="line"><span class="meta">    auto s52 = _mm_loadu_ps(srcX + 22 * pOffset); \</span></span><br><span class="line"><span class="meta">    auto s53 = _mm_loadu_ps(srcX + 23 * pOffset); \</span></span><br><span class="line"><span class="meta">    _MM_TRANSPOSE4_PS(s00, s01, s02, s03);  \</span></span><br><span class="line"><span class="meta">    _MM_TRANSPOSE4_PS(s10, s11, s12, s13);  \</span></span><br><span class="line"><span class="meta">    _MM_TRANSPOSE4_PS(s20, s21, s22, s23);  \</span></span><br><span class="line"><span class="meta">    _MM_TRANSPOSE4_PS(s30, s31, s32, s33);  \</span></span><br><span class="line"><span class="meta">    _MM_TRANSPOSE4_PS(s40, s41, s42, s43);  \</span></span><br><span class="line"><span class="meta">    _MM_TRANSPOSE4_PS(s50, s51, s52, s53);</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> STORE_TEMP(i)                               \</span></span><br><span class="line"><span class="meta">    _mm_storeu_ps(dstX + 4 * (6 * i + 0), s##0##i); \</span></span><br><span class="line"><span class="meta">    _mm_storeu_ps(dstX + 4 * (6 * i + 1), s##1##i); \</span></span><br><span class="line"><span class="meta">    _mm_storeu_ps(dstX + 4 * (6 * i + 2), s##2##i); \</span></span><br><span class="line"><span class="meta">    _mm_storeu_ps(dstX + 4 * (6 * i + 3), s##3##i); \</span></span><br><span class="line"><span class="meta">    _mm_storeu_ps(dstX + 4 * (6 * i + 4), s##4##i); \</span></span><br><span class="line"><span class="meta">    _mm_storeu_ps(dstX + 4 * (6 * i + 5), s##5##i);</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> x = <span class="number">0</span>; x &lt; lC4; ++x) &#123;</span><br><span class="line">    <span class="keyword">auto</span> srcX = source + x * <span class="number">4</span> * eReal;</span><br><span class="line">    <span class="keyword">auto</span> dstX = dest + x * eDest * <span class="number">4</span>;</span><br><span class="line">    MAIN_COMPUTE;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">STORE_TEMP</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="built_in">STORE_TEMP</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="built_in">STORE_TEMP</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="built_in">STORE_TEMP</span>(<span class="number">3</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>用图展示如下：</p>
<p><img src="4.png" alt="4.png"></p>
<p>即属于同一个输入通道的24个元素排在一起，两个<code>C4 Pack</code>通道处理完就是下面这样：</p>
<p><img src="5.png" alt="5.png"></p>
<h2 id="MNNPackedMatMul">MNNPackedMatMul</h2>
<blockquote>
<p>同样是AVX加速版本，这里贴出的是release_1.2.3版本，与1.1.7版本相比，逻辑没有变化，只是函数名由<code>_AVX_MNNPackedMatMul_24</code>改为<code>_AVX_MNNPackedMatMul_Main</code></p>
</blockquote>
<p><code>_AVX_MNNPackedMatMul_Main</code>函数代码如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INIT_MAIN_24_4                                  \</span></span><br><span class="line"><span class="meta">    auto s0  = LOAD8(A + 0 * 24);             \</span></span><br><span class="line"><span class="meta">    auto s1  = LOAD8(A + 0 * 24 + 8);         \</span></span><br><span class="line"><span class="meta">    auto s2  = LOAD8(A + 0 * 24 + 16);        \</span></span><br><span class="line"><span class="meta">    auto w0  = BROAD_LOAD(weight + 0 * 4 + 0); \</span></span><br><span class="line"><span class="meta">    auto z0  = _mm256_mul_ps(s0, w0);                   \</span></span><br><span class="line"><span class="meta">    auto z1  = _mm256_mul_ps(s1, w0);                   \</span></span><br><span class="line"><span class="meta">    auto z2  = _mm256_mul_ps(s2, w0);                   \</span></span><br><span class="line"><span class="meta">    auto w1  = BROAD_LOAD(weight + 0 * 4 + 1); \</span></span><br><span class="line"><span class="meta">    auto z3  = _mm256_mul_ps(s0, w1);                   \</span></span><br><span class="line"><span class="meta">    auto z4  = _mm256_mul_ps(s1, w1);                   \</span></span><br><span class="line"><span class="meta">    auto z5  = _mm256_mul_ps(s2, w1);                   \</span></span><br><span class="line"><span class="meta">    auto w2  = BROAD_LOAD(weight + 0 * 4 + 2); \</span></span><br><span class="line"><span class="meta">    auto z6  = _mm256_mul_ps(s0, w2);                   \</span></span><br><span class="line"><span class="meta">    auto z7  = _mm256_mul_ps(s1, w2);                   \</span></span><br><span class="line"><span class="meta">    auto z8  = _mm256_mul_ps(s2, w2);                   \</span></span><br><span class="line"><span class="meta">    auto w3  = BROAD_LOAD(weight + 0 * 4 + 3); \</span></span><br><span class="line"><span class="meta">    auto z9  = _mm256_mul_ps(s0, w3);                   \</span></span><br><span class="line"><span class="meta">    auto z10 = _mm256_mul_ps(s1, w3);                   \</span></span><br><span class="line"><span class="meta">    auto z11 = _mm256_mul_ps(s2, w3);</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> COMPUTE_24_4                                \</span></span><br><span class="line"><span class="meta">    s0  = LOAD8(A + sy * 24);             \</span></span><br><span class="line"><span class="meta">    s1  = LOAD8(A + sy * 24 + 8);         \</span></span><br><span class="line"><span class="meta">    s2  = LOAD8(A + sy * 24 + 16);        \</span></span><br><span class="line"><span class="meta">    w0  = BROAD_LOAD(weight + sy * 4 + 0); \</span></span><br><span class="line"><span class="meta">    z0  = MNNAVXFMA(s0, w0, z0);                    \</span></span><br><span class="line"><span class="meta">    z1  = MNNAVXFMA(s1, w0, z1);                    \</span></span><br><span class="line"><span class="meta">    z2  = MNNAVXFMA(s2, w0, z2);                    \</span></span><br><span class="line"><span class="meta">    w1  = BROAD_LOAD(weight + sy * 4 + 1); \</span></span><br><span class="line"><span class="meta">    z3  = MNNAVXFMA(s0, w1, z3);                    \</span></span><br><span class="line"><span class="meta">    z4  = MNNAVXFMA(s1, w1, z4);                    \</span></span><br><span class="line"><span class="meta">    z5  = MNNAVXFMA(s2, w1, z5);                    \</span></span><br><span class="line"><span class="meta">    w2  = BROAD_LOAD(weight + sy * 4 + 2); \</span></span><br><span class="line"><span class="meta">    z6  = MNNAVXFMA(s0, w2, z6);                    \</span></span><br><span class="line"><span class="meta">    z7  = MNNAVXFMA(s1, w2, z7);                    \</span></span><br><span class="line"><span class="meta">    z8  = MNNAVXFMA(s2, w2, z8);                    \</span></span><br><span class="line"><span class="meta">    w3  = BROAD_LOAD(weight + sy * 4 + 3); \</span></span><br><span class="line"><span class="meta">    z9  = MNNAVXFMA(s0, w3, z9);                    \</span></span><br><span class="line"><span class="meta">    z10 = MNNAVXFMA(s1, w3, z10);                   \</span></span><br><span class="line"><span class="meta">    z11 = MNNAVXFMA(s2, w3, z11);</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> TYPE&gt;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> _AVX_MNNPackedMatMul_Main(TYPE* C, <span class="keyword">const</span> TYPE* A, <span class="keyword">const</span> TYPE* B, <span class="keyword">const</span> <span class="keyword">size_t</span>* parameter) &#123;</span><br><span class="line">    <span class="keyword">auto</span> h            = parameter[<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">auto</span> l            = parameter[<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">auto</span> cStride      = parameter[<span class="number">3</span>] / <span class="built_in"><span class="keyword">sizeof</span></span>(TYPE);</span><br><span class="line">    <span class="keyword">auto</span> bExtraStride = parameter[<span class="number">5</span>] / <span class="built_in"><span class="keyword">sizeof</span></span>(TYPE);</span><br><span class="line">    <span class="keyword">auto</span> bStride      = bExtraStride + l * <span class="number">4</span>;</span><br><span class="line">    <span class="keyword">auto</span> hC4          = <span class="built_in">UP_DIV</span>(h, <span class="number">4</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> y = <span class="number">0</span>; y &lt; hC4; ++y) &#123;</span><br><span class="line">        <span class="keyword">auto</span> weight = B + y * bStride;</span><br><span class="line">        <span class="keyword">auto</span> dst    = C + (y / <span class="number">2</span>) * cStride + <span class="number">4</span> * (y % <span class="number">2</span>);</span><br><span class="line">        INIT_MAIN_24_4;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> sy = <span class="number">1</span>; sy &lt; l; ++sy) &#123;</span><br><span class="line">            COMPUTE_24_4;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">TRANPOSE_SAVE</span>(<span class="number">0</span>, <span class="number">0</span>, z0, z3, z6, z9);</span><br><span class="line">        <span class="built_in">TRANPOSE_SAVE</span>(<span class="number">1</span>, <span class="number">0</span>, z0, z3, z6, z9);</span><br><span class="line">        <span class="built_in">TRANPOSE_SAVE</span>(<span class="number">0</span>, <span class="number">1</span>, z1, z4, z7, z10);</span><br><span class="line">        <span class="built_in">TRANPOSE_SAVE</span>(<span class="number">1</span>, <span class="number">1</span>, z1, z4, z7, z10);</span><br><span class="line">        <span class="built_in">TRANPOSE_SAVE</span>(<span class="number">0</span>, <span class="number">2</span>, z2, z5, z8, z11);</span><br><span class="line">        <span class="built_in">TRANPOSE_SAVE</span>(<span class="number">1</span>, <span class="number">2</span>, z2, z5, z8, z11);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面详细看下计算过程：</p>
<p>首先看下将要参与运算的数据排布形式是怎么样的，我们将上面<strong>权重变换</strong>后（取<code>8x8</code>中的一个位置，其余循环64次即可）的图 和 <strong>输入变换</strong>后（取一个<code>tile</code>中<code>64 x 24 x 2</code>中的一行，即<code>24 x 2</code> （按照<code>C4 Pack</code>展开后就是<code>24 x 8</code>），其余循环64次即可）的图放在一起：</p>
<p><img src="6.png" alt="6.png"></p>
<p>乘加工作就是要在上面的两个图展开。为了解释起来更简单，再从上面权重图中<strong>拿1个<code>8 x C4 Pack</code>（即：4个卷积核）来</strong>，至于所有的4个<code>8 x C4 Pack</code>（即：16个卷积核）卷积核循环4次处理即可。取出来的<code>Mul+Add</code>运算图如下：</p>
<p><img src="7.png" alt="7.png"></p>
<p>上图中的计算流程归纳一下：</p>
<ul>
<li>
<ol>
<li><code>MUL</code>操作时，以上图中两个黑色框为计算单元。输入的黑色框中每次取一个值出来，与权重第一行黑色框中4个值依次相乘，并将结果pack到一起，直到24个值全部计算完成，输出一行<code>24 (x 4)</code>。</li>
</ol>
</li>
<li>
<ol start="2">
<li>输入更新到下一个通道24个值，权重也下移一行，重复8次上述运算。得到<code>8 x 24 (x 4)</code>。</li>
</ol>
</li>
<li>
<ol start="3">
<li>将8行数据对应位置累加(即：同一个卷积核的不同通道累加)，得到24个点一个<code>C4 Pack</code>的卷积结果：<code>24 (x 4)</code>。</li>
</ol>
</li>
<li>
<ol start="4">
<li>卷积核有16个，即：4个<code>C4 Pack</code>，循环4次，将所有卷积核处理完，得到24个点所有卷积核的卷积结果：<code>24 (x 4) x 4</code></li>
</ol>
</li>
</ul>
<p>至此，一个<code>tile</code>的<strong>一行</strong>数据处理完成，循环64次，就把整个<code>tile</code>的数据全部计算完成。并仍旧按照64 × <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Green"><mn>24</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Green}24</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Green;">24</span></span></span></span> × (× <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Blue"><mn>4</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Blue}4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Blue;">4</span></span></span></span>) × <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Red"><mn>4</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Red}4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Red;">4</span></span></span></span>（这里把<code>C4 Pack</code>的通道数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Red"><mn>4</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Red}4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Red;">4</span></span></span></span> 放到最后，是为了便于理解其跟Pack 的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Blue"><mn>4</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Blue}4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Blue;">4</span></span></span></span>是同一维度的，虽然在数据排布上，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Red"><mn>4</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Red}4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Red;">4</span></span></span></span>应该在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="Green"><mn>24</mn></mstyle></mrow><annotation encoding="application/x-tex">\color{Green}24</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord" style="color:Green;">24</span></span></span></span>之前）的方式放入内存中。一个tile全部处理完后的排布如图：</p>
<p><img src="8.png" alt="8.png"></p>
<p>为了直观方便对比，我们把乘加前和乘加后的图放在一起再看一遍：</p>
<p><img src="3.png" alt="3.png"></p>
<p><img src="8.png" alt="8.png"></p>
<h1>输出变换</h1>
<p>单个<code>tile</code>的<code>MatMul + ADD</code> 计算完成后，需要根据输出变换矩阵计算得到最终的卷积输出结果。其过程仍旧和输入变换类似，核心代码如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> z = <span class="number">0</span>; z &lt; dc_4; ++z) &#123;</span><br><span class="line">    <span class="keyword">auto</span> dstZAddr = dstStart + z * dstZStep * bytes;</span><br><span class="line">    <span class="keyword">auto</span> srcZ     = srcXi + z * srcZStep * bytes;</span><br><span class="line">    <span class="comment">// Transform</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; srcUnit; ++i) &#123;</span><br><span class="line">        <span class="keyword">auto</span> srcFloatPtr = (<span class="keyword">const</span> <span class="keyword">float</span>*)(srcZ + i * unitStep * bytes);</span><br><span class="line">        <span class="keyword">auto</span> dstFloatPtr = (<span class="keyword">float</span>*)(midBuffer0 + i * dstUnit * pack * bytes);</span><br><span class="line">        <span class="built_in">mDestTransform</span>(srcFloatPtr, dstFloatPtr, srcUnit * unitStep, pack);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; ey; ++i) &#123;</span><br><span class="line">        <span class="keyword">auto</span> srcFloatPtr = (<span class="keyword">const</span> <span class="keyword">float</span>*)(midBuffer0 + i * pack * bytes);</span><br><span class="line">        <span class="keyword">auto</span> dstFloatPtr = (<span class="keyword">float</span>*)(dstZAddr + i * pack * ow * bytes);</span><br><span class="line">        <span class="built_in">mDestTransform</span>(srcFloatPtr, dstFloatPtr, pack * dstUnit, pack);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从代码可以看到，依旧是执行了两次<code>mDestTransform</code>函数，同输入变换一样，MNN对输出变换也做了如下的等价调整：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>A</mi><mi>T</mi></msup><mi>O</mi><mi>A</mi><mo>=</mo><mo stretchy="false">(</mo><msup><mi>O</mi><mi>T</mi></msup><mi>A</mi><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mi>A</mi></mrow><annotation encoding="application/x-tex">A^TOA=(O^TA)^TA
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8913em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal">A</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal">A</span></span></span></span></span></p>
<p><code>mDestTransform</code>函数同样基于<code>unit</code>而有不同的实现，这里贴出<code>8 x 6</code>的实现：</p>
<blockquote>
<p>同样为了保持逻辑连贯性，此处采用Vec4版本实现，AVX可以使用Vec8版本实现</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> _destTransformUnit8x6(<span class="keyword">const</span> <span class="keyword">float</span>* srcBlock, <span class="keyword">float</span>* dstStart, <span class="keyword">size_t</span> srcStep, <span class="keyword">size_t</span> dstStep) &#123;</span><br><span class="line">    Vec4 s0 = Vec4::<span class="built_in">load</span>(srcBlock + <span class="number">0</span> * srcStep);</span><br><span class="line">    Vec4 s1 = Vec4::<span class="built_in">load</span>(srcBlock + <span class="number">1</span> * srcStep);</span><br><span class="line">    Vec4 s2 = Vec4::<span class="built_in">load</span>(srcBlock + <span class="number">2</span> * srcStep);</span><br><span class="line">    Vec4 s3 = Vec4::<span class="built_in">load</span>(srcBlock + <span class="number">3</span> * srcStep);</span><br><span class="line">    Vec4 s4 = Vec4::<span class="built_in">load</span>(srcBlock + <span class="number">4</span> * srcStep);</span><br><span class="line">    Vec4 s5 = Vec4::<span class="built_in">load</span>(srcBlock + <span class="number">5</span> * srcStep);</span><br><span class="line">    Vec4 s6 = Vec4::<span class="built_in">load</span>(srcBlock + <span class="number">6</span> * srcStep);</span><br><span class="line">    Vec4 s7 = Vec4::<span class="built_in">load</span>(srcBlock + <span class="number">7</span> * srcStep);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> m0 = s0 + s1 + s2 + s3 + s4 + s5 + s6;</span><br><span class="line">    <span class="keyword">auto</span> m1 = (s1 - s2) + (s3 - s4) * <span class="number">2.f</span> + (s5 - s6) * <span class="number">3.f</span>;</span><br><span class="line">    <span class="keyword">auto</span> m2 = (s1 + s2) + (s3 + s4) * <span class="number">4.f</span> + (s5 + s6) * <span class="number">9.f</span>;</span><br><span class="line">    <span class="keyword">auto</span> m3 = (s1 - s2) + (s3 - s4) * <span class="number">8.f</span> + (s5 - s6) * <span class="number">27.f</span>;</span><br><span class="line">    <span class="keyword">auto</span> m4 = (s1 + s2) + (s3 + s4) * <span class="number">16.f</span> + (s5 + s6) * <span class="number">81.f</span>;</span><br><span class="line">    <span class="keyword">auto</span> m5 = (s1 - s2) + (s3 - s4) * <span class="number">32.f</span> + (s5 - s6) * <span class="number">243.f</span> + s7;</span><br><span class="line"></span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">0</span> * dstStep, m0);</span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">1</span> * dstStep, m1);</span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">2</span> * dstStep, m2);</span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">3</span> * dstStep, m3);</span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">4</span> * dstStep, m4);</span><br><span class="line">    Vec4::<span class="built_in">save</span>(dstStart + <span class="number">5</span> * dstStep, m5);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里的转置操作实际隐藏在<code>load</code>操作中，因为<code>srcStep=srcUnit * unitStep</code>，会导致跳行读取。</p>
<p>一次<code>mDestTransform</code>处理一行（8个<code>C4 Pack</code>），循环<code>srcUnit=8</code>次得到<code>8 x 6 x (x 4) </code>的中间结果<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>O</mi><mi>T</mi></msup><mi>A</mi></mrow><annotation encoding="application/x-tex">O^TA</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal">A</span></span></span></span>，再循环<code>ey=6</code>次得到<code>6 x 6 x (x 4)</code>的最终结果。</p>
<p>然后循环4次，把4个通道也处理完毕。然后循环24次，把一个tile处理完毕。然后处理其他tile，58个tile全部处理完，则完整整个输入的卷积操作。</p>
<p><img src="9.png" alt="9.png"></p>
<h1>PostTransform (Bias, Relu, Relu6)</h1>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">MNN_CONCURRENCY_BEGIN</span>(tId, threadNumber) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> dy=(<span class="keyword">int</span>)tId; dy &lt; dc_4; dy += threadNumber) &#123;</span><br><span class="line">        <span class="keyword">auto</span> dataFloatPtr = (<span class="keyword">float</span>*)(dstOrigin + ow * oh * batch * dy * pack * bytes);</span><br><span class="line">        <span class="keyword">auto</span> biasFloatPtr = (<span class="keyword">const</span> <span class="keyword">float</span>*)(bias + pack * dy * bytes);</span><br><span class="line">        core-&gt;<span class="built_in">MNNAxByClampBroadcastUnit</span>(dataFloatPtr, dataFloatPtr, biasFloatPtr, ow * oh * batch, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>,  mPostParameters.<span class="built_in">data</span>());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">MNN_CONCURRENCY_END</span>();</span><br></pre></td></tr></table></figure>
<p><code>core-&gt;MNNAxByClampBroadcastUnit</code>的AVX实现为<code>_AVX_MNNAxByClampBroadcastUnit</code>函数，代码如下：</p>
<blockquote>
<p>这里逻辑比较简单，就不放之前版本的<code>C4 Pack</code>实现了，这里是<code>C8 Pack</code>实现</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">void</span> _AVX_MNNAxByClampBroadcastUnit(<span class="keyword">float</span>* C, <span class="keyword">const</span> <span class="keyword">float</span>* A, <span class="keyword">const</span> <span class="keyword">float</span>* B, <span class="keyword">size_t</span> width, <span class="keyword">size_t</span> cStride, <span class="keyword">size_t</span> aStride, <span class="keyword">size_t</span> height, <span class="keyword">const</span> <span class="keyword">float</span>* parameters) &#123;</span><br><span class="line">    <span class="keyword">auto</span> minF = _mm256_broadcast_ss(parameters + <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">auto</span> maxF = _mm256_broadcast_ss(parameters + <span class="number">3</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> y = <span class="number">0</span>; y &lt; height; ++y) &#123;</span><br><span class="line">        <span class="keyword">auto</span> a = A + aStride * y;</span><br><span class="line">        <span class="keyword">auto</span> b = B + PACK_UNIT * y;</span><br><span class="line">        <span class="keyword">auto</span> bv = _mm256_loadu_ps(b);</span><br><span class="line">        <span class="keyword">auto</span> c = C + cStride * y;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> x = <span class="number">0</span>; x &lt; width; ++x) &#123;</span><br><span class="line">            <span class="keyword">auto</span> av = _mm256_loadu_ps(a);</span><br><span class="line">            <span class="keyword">auto</span> cv = _mm256_add_ps(av, bv);</span><br><span class="line">            cv = _mm256_min_ps(cv, maxF);</span><br><span class="line">            cv = _mm256_max_ps(cv, minF);</span><br><span class="line">            _mm256_storeu_ps(c, cv);</span><br><span class="line">            a += PACK_UNIT;</span><br><span class="line">            c += PACK_UNIT;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每<code>C8 Pack</code>统一加上一组<code>bias</code>，再根据<code>minF/maxF</code>(在模型初始化时就根据是relu还是relu6来确定，放到<code>mPostParameters</code>中了)去处理激活层。</p>
<h1>致谢</h1>
<p>文章主体框架参考自<a target="_blank" rel="noopener" href="https://github.com/yizhaoyanbo">东哥</a>的MNN源码解读的内部分享，加上了自己的一些看法。有幸被看到的话，希望能给点个赞~~</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">旭穹</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://no5-aaron-wu.github.io/2021/11/16/AI-Algorithm-5-WinogradInMnn/">https://no5-aaron-wu.github.io/2021/11/16/AI-Algorithm-5-WinogradInMnn/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://no5-aaron-wu.github.io" target="_blank">旭穹の陋室</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/Algorithm/">Algorithm</a><a class="post-meta__tags" href="/tags/MNN/">MNN</a><a class="post-meta__tags" href="/tags/Winograd/">Winograd</a></div><div class="post_share"><div class="social-share" data-image="/images/%E6%88%98%E5%8F%8CCG_%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%97.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/images/wechatpay.png" target="_blank"><img class="post-qr-code-img" src="/images/wechatpay.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/images/alipay.png" target="_blank"><img class="post-qr-code-img" src="/images/alipay.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/11/18/AI-Algorithm-6-Strassen/"><img class="prev-cover" src="/images/%E6%88%98%E5%8F%8CCG_%E9%87%8A%E7%84%B6.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">AI算法基础 [6]：Strassen算法原理</div></div></a></div><div class="next-post pull-right"><a href="/2021/11/16/AI-Algorithm-4-Winograd/"><img class="next-cover" src="/images/%E6%88%98%E5%8F%8CCG_%E6%8C%A3%E6%89%8E.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">AI算法基础 [4]：Winograd算法原理</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/11/22/AI-Algorithm-8-ConvTiledInMnn/" title="AI算法基础 [8]：MNN中的ConvolutionTiled实现"><img class="cover" src="/images/%E6%88%98%E5%8F%8CCG_%E6%88%98%E5%9C%BA%E4%B9%8B%E8%8A%B1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-22</div><div class="title">AI算法基础 [8]：MNN中的ConvolutionTiled实现</div></div></a></div><div><a href="/2021/11/18/AI-Algorithm-7-StrassenInMnn/" title="AI算法基础 [7]：MNN中的Strassen实现"><img class="cover" src="/images/%E6%88%98%E5%8F%8CCG_%E4%B8%8D%E9%80%9F%E4%B9%8B%E5%AE%A2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-18</div><div class="title">AI算法基础 [7]：MNN中的Strassen实现</div></div></a></div><div><a href="/2021/11/16/AI-Algorithm-4-Winograd/" title="AI算法基础 [4]：Winograd算法原理"><img class="cover" src="/images/%E6%88%98%E5%8F%8CCG_%E6%8C%A3%E6%89%8E.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-16</div><div class="title">AI算法基础 [4]：Winograd算法原理</div></div></a></div><div><a href="/2021/11/18/AI-Algorithm-6-Strassen/" title="AI算法基础 [6]：Strassen算法原理"><img class="cover" src="/images/%E6%88%98%E5%8F%8CCG_%E9%87%8A%E7%84%B6.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-18</div><div class="title">AI算法基础 [6]：Strassen算法原理</div></div></a></div><div><a href="/2021/11/11/AI-Algorithm-1-mIoU/" title="AI算法基础 [1]：mIoU"><img class="cover" src="/images/%E6%88%98%E5%8F%8CCG_%E9%B8%A0%E5%8D%A0%E9%B9%8A%E5%B7%A2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-11</div><div class="title">AI算法基础 [1]：mIoU</div></div></a></div><div><a href="/2021/11/14/AI-Algorithm-2-NC4HW4/" title="AI算法基础 [2]：NC4HW4数据排布"><img class="cover" src="/images/%E6%88%98%E5%8F%8CCG_%E5%86%B3%E6%84%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-14</div><div class="title">AI算法基础 [2]：NC4HW4数据排布</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">旭穹</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">15</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/no5-aaron-wu"><i class="fab fa-github"></i><span>我的github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="mailto:no5aaron@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">Winograd适用条件</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">选取最佳的Unit</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">生成变换矩阵</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">权重变换</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">分块处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%9C%AF%E8%AF%AD"><span class="toc-number">6.1.</span> <span class="toc-text">常用术语</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%9D%97%E5%8E%9F%E7%90%86"><span class="toc-number">6.2.</span> <span class="toc-text">分块原理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">7.</span> <span class="toc-text">buffer分配</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">8.</span> <span class="toc-text">输入变换</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">9.</span> <span class="toc-text">MatMul + Add 乘加计算</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MNNPackC4ForMatMul-A"><span class="toc-number">9.1.</span> <span class="toc-text">MNNPackC4ForMatMul_A</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MNNPackedMatMul"><span class="toc-number">9.2.</span> <span class="toc-text">MNNPackedMatMul</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">10.</span> <span class="toc-text">输出变换</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">11.</span> <span class="toc-text">PostTransform (Bias, Relu, Relu6)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">12.</span> <span class="toc-text">致谢</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/11/30/CUDA-5-mutexLock/" title="CUDA基础 [5]：互斥锁设计"><img src="/images/%E6%88%98%E5%8F%8CCG_%E6%9C%8B%E5%8F%8B%E4%B9%8B%E9%97%B4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CUDA基础 [5]：互斥锁设计"/></a><div class="content"><a class="title" href="/2021/11/30/CUDA-5-mutexLock/" title="CUDA基础 [5]：互斥锁设计">CUDA基础 [5]：互斥锁设计</a><time datetime="2021-11-30T14:50:37.000Z" title="发表于 2021-11-30 14:50:37">2021-11-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/11/28/CUDA-4-MultiStreamTest/" title="CUDA基础 [4]：多流测试"><img src="/images/%E6%88%98%E5%8F%8CCG_%E6%9F%90%E6%97%A5%E7%9A%84%E5%90%AF%E7%A8%8B.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CUDA基础 [4]：多流测试"/></a><div class="content"><a class="title" href="/2021/11/28/CUDA-4-MultiStreamTest/" title="CUDA基础 [4]：多流测试">CUDA基础 [4]：多流测试</a><time datetime="2021-11-28T17:03:57.000Z" title="发表于 2021-11-28 17:03:57">2021-11-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/11/28/CUDA-3-StreamAndEvent/" title="CUDA基础 [3]：流和事件"><img src="/images/%E6%88%98%E5%8F%8CCG_%E6%9F%90%E6%97%A5%E7%9A%84%E7%9C%9F%E7%9B%B8.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CUDA基础 [3]：流和事件"/></a><div class="content"><a class="title" href="/2021/11/28/CUDA-3-StreamAndEvent/" title="CUDA基础 [3]：流和事件">CUDA基础 [3]：流和事件</a><time datetime="2021-11-28T14:25:14.000Z" title="发表于 2021-11-28 14:25:14">2021-11-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/11/27/CUDA-2-start/" title="CUDA基础 [2]：Get Started"><img src="/images/%E6%88%98%E5%8F%8CCG_%E6%9F%90%E6%9C%88%E6%9F%90%E6%97%A5%E6%99%B4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CUDA基础 [2]：Get Started"/></a><div class="content"><a class="title" href="/2021/11/27/CUDA-2-start/" title="CUDA基础 [2]：Get Started">CUDA基础 [2]：Get Started</a><time datetime="2021-11-27T20:47:29.000Z" title="发表于 2021-11-27 20:47:29">2021-11-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/11/27/CUDA-1-xPU/" title="CUDA基础 [1]：CPU GPU TPU NPU"><img src="/images/%E6%88%98%E5%8F%8CCG_%E7%BA%AF%E7%99%BD%E7%9A%84%E7%A4%BC%E6%9C%8D.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CUDA基础 [1]：CPU GPU TPU NPU"/></a><div class="content"><a class="title" href="/2021/11/27/CUDA-1-xPU/" title="CUDA基础 [1]：CPU GPU TPU NPU">CUDA基础 [1]：CPU GPU TPU NPU</a><time datetime="2021-11-27T16:49:10.000Z" title="发表于 2021-11-27 16:49:10">2021-11-27</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/images/%E6%88%98%E5%8F%8CCG_%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%97.png')"><div id="footer-wrap"><div class="copyright">&copy;2021 By 旭穹</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">谢临<a href="https://no5-aaron-wu.github.io/">陋室</a>, 欢迎留言！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script async="async">var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> {preloader.endLoading()})
setTimeout(function(){preloader.endLoading();}, 3000);</script></div><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '8238e4a92ed02fb5dcb8',
      clientSecret: '9c0dfb8fd7077aa7549fbda345a3b0c2cb781947',
      repo: 'no5-aaron-wu.github.io',
      owner: 'no5-aaron-wu',
      admin: ['no5-aaron-wu'],
      id: '371506a70ba432383a54210288245aa1',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script><script src="/js/ginkgo-leaf.js"></script><script src="/js/footer.js"></script><script src="/js/fishes.js"></script><script src="/live2d-widget/autoload.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>